{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# adulib.llm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|default_exp llm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "import nblite; from nbdev.showdoc import show_doc; nblite.nbl_export()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "try:\n",
                "    import os\n",
                "    import json\n",
                "    import tempfile\n",
                "    from pathlib import Path\n",
                "    from openai import OpenAI, AsyncOpenAI\n",
                "    import tiktoken\n",
                "    from asynciolimiter import Limiter\n",
                "    import diskcache\n",
                "    from typing import List, Optional, Type, Dict, Any\n",
                "    import asyncio\n",
                "    import time\n",
                "    from tqdm import tqdm\n",
                "except ImportError as e:\n",
                "    raise ImportError(f\"Install adulib[{__name__.split('.')[-1]}] to use this API.\") from e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from dotenv import load_dotenv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import adulib.llm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "load_dotenv()\n",
                "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "model_context_windows = {\n",
                "    'gpt-4o': 128000 * 0.8,\n",
                "    'gpt-4o-mini': 128000 * 0.8,\n",
                "}\n",
                "\n",
                "model_rate_limits = {\n",
                "    'gpt-4o': 10000,\n",
                "    'gpt-4o-mini': 30000,\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "show_doc(adulib.llm.tokenize_text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "__model_tokenisers = {}\n",
                "\n",
                "def tokenize_text(text, llm_model):\n",
                "    if llm_model not in __model_tokenisers:\n",
                "        __model_tokenisers[llm_model] = tiktoken.encoding_for_model(llm_model)\n",
                "    return __model_tokenisers[llm_model].encode(text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tokenize_text(\"hello world\", 'gpt-4o')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "show_doc(adulib.llm.detokenize_text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "def detokenize_text(tokens, llm_model):\n",
                "    if llm_model not in __model_tokenisers:\n",
                "        __model_tokenisers[llm_model] = tiktoken.encoding_for_model(llm_model)\n",
                "    tokeniser_enc = tiktoken.encoding_for_model(llm_model)\n",
                "    return tokeniser_enc.decode(tokens)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "llm_model = 'gpt-4o'\n",
                "detokenize_text(tokenize_text(\"hello world\", llm_model), llm_model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "show_doc(adulib.llm.get_token_count)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "def get_token_count(text, llm_model):\n",
                "    return len(tokenize_text(text, llm_model))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "get_token_count(\"hello world\", \"gpt-4o\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "show_doc(adulib.llm.async_prompt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "__client = None\n",
                "__model_rate_limiters = {}\n",
                "__llm_cache = None\n",
                "\n",
                "async def async_prompt(model,\n",
                "                           prompt,\n",
                "                           context=\"You are a helpful assistant.\",\n",
                "                           api_key=None,\n",
                "                           response_format=None,\n",
                "                           use_cache=True,\n",
                "                           cache_dir=None,\n",
                "                           include_model_in_cache_key=False,\n",
                "                           cache_key_prepend=''):\n",
                "    global __client, __llm_cache\n",
                "    if __client is None:\n",
                "        if api_key is None: api_key = os.environ.get(\"PROJ_OPENAI_API_KEY\")\n",
                "        __client = AsyncOpenAI(api_key=api_key)\n",
                "        \n",
                "    if model not in __model_rate_limiters:\n",
                "        __model_rate_limiters[model] = Limiter(model_rate_limits[model]/60)\n",
                "        \n",
                "    response_schema = str(response_format.model_json_schema()) if response_format else \"\"\n",
                "        \n",
                "    if __llm_cache is None:\n",
                "        if cache_dir is None: cache_dir = tempfile.mkdtemp()\n",
                "        abs_cache_path = Path(cache_dir).resolve().as_posix()\n",
                "        __llm_cache = diskcache.Cache(abs_cache_path, eviction_policy=\"none\", size_limit=2**40)\n",
                "    \n",
                "    _model_key = model if include_model_in_cache_key else '*'\n",
                "    cache_key = f'{cache_key_prepend}:{_model_key}:{prompt}:{context}:{response_schema}'\n",
                "    \n",
                "    if use_cache and cache_key in __llm_cache:\n",
                "        output =  __llm_cache[cache_key]\n",
                "    else:\n",
                "        await __model_rate_limiters[model].wait()\n",
                "        if not response_schema:\n",
                "            chat_completion = await __client.chat.completions.create(\n",
                "                messages=[\n",
                "                    {\n",
                "                        \"role\": \"system\",\n",
                "                        \"content\": context\n",
                "                    },\n",
                "                    {\n",
                "                        \"role\": \"user\",\n",
                "                        \"content\": prompt,\n",
                "                    }\n",
                "                ],\n",
                "                model=model,\n",
                "            )\n",
                "        else:\n",
                "            chat_completion = await __client.beta.chat.completions.parse(\n",
                "                messages=[\n",
                "                    {\n",
                "                        \"role\": \"system\",\n",
                "                        \"content\": context\n",
                "                    },\n",
                "                    {\n",
                "                        \"role\": \"user\",\n",
                "                        \"content\": prompt,\n",
                "                    }\n",
                "                ],\n",
                "                model=model,\n",
                "                response_format=response_format\n",
                "            )\n",
                "        output = chat_completion.choices[0].message.content\n",
                "        __llm_cache[cache_key] = output\n",
                "    \n",
                "    if response_format:\n",
                "        return response_format(**json.loads(output))\n",
                "    else:\n",
                "        return output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "await async_prompt(\n",
                "    model='gpt-4o',\n",
                "    context='You are a helpful assistant.',\n",
                "    prompt='Hello, how are you?',\n",
                "    cache_dir='./tmp/llm_cache'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pydantic import BaseModel\n",
                "from pprint import pprint"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Step(BaseModel):\n",
                "    explanation: str\n",
                "    output: str\n",
                "\n",
                "class MathReasoning(BaseModel):\n",
                "    steps: list[Step]\n",
                "    final_answer: str\n",
                "    \n",
                "res = await async_prompt(\n",
                "    model='gpt-4o',\n",
                "    context='You are a helpful math tutor. Guide the user through the solution step by step.',\n",
                "    prompt='How can I solve 8x + 7 = -23?',\n",
                "    response_format=MathReasoning,\n",
                "    cache_dir='./tmp/llm_cache'\n",
                ")\n",
                "\n",
                "pprint(res.model_dump())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "###\u00a0async_prompts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "async def async_prompts(\n",
                "    model: str,\n",
                "    context: str,\n",
                "    items: List[str],\n",
                "    prompt_template: str,\n",
                "    response_format: Optional[Type[BaseModel]] = None,\n",
                "    api_key: Optional[str] = None,\n",
                "    cache_dir: Optional[str] = None,\n",
                "    include_model_in_cache_key: bool = False,\n",
                "    cache_key_prepend: str = '',\n",
                "    max_retries: int = 3,\n",
                "    timeout: int = 30,\n",
                "    time_window = 20,\n",
                "    concurrency_limit: Optional[int] = None\n",
                "    ):\n",
                "    semaphore = asyncio.Semaphore(concurrency_limit) if concurrency_limit else asyncio.Semaphore(len(items))\n",
                "    \n",
                "    async def process_item(item: str):\n",
                "        prompt = prompt_template.format(item=item)\n",
                "\n",
                "        for attempt in range(max_retries):\n",
                "            async with semaphore:\n",
                "                start_time = time.time()\n",
                "                try:\n",
                "                    task = asyncio.create_task(\n",
                "                        async_prompt(\n",
                "                            model=model,\n",
                "                            prompt=prompt,\n",
                "                            context=context,\n",
                "                            response_format=response_format,\n",
                "                            api_key=api_key,\n",
                "                            use_cache=True,\n",
                "                            cache_dir=cache_dir,\n",
                "                            include_model_in_cache_key=include_model_in_cache_key,\n",
                "                            cache_key_prepend=cache_key_prepend\n",
                "                        )\n",
                "                    )\n",
                "\n",
                "                    done, pending = await asyncio.wait({task}, timeout=timeout)\n",
                "\n",
                "                    if task in done:\n",
                "                        return task.result()\n",
                "\n",
                "                    else:\n",
                "                        task.cancel()\n",
                "                        print(f\"[Timeout] '{item}' attempt {attempt + 1}/{max_retries} exceeded {timeout}s\")\n",
                "\n",
                "                except asyncio.CancelledError:\n",
                "                    print(f\"[Cancelled] '{item}' attempt {attempt + 1}/{max_retries}\")\n",
                "\n",
                "                except Exception as e:\n",
                "                    print(f\"[Error] '{item}' attempt {attempt + 1}/{max_retries}: {e}\")\n",
                "\n",
                "                await asyncio.sleep(2 ** attempt)  # exponential backoff\n",
                "\n",
                "                elapsed = time.time() - start_time\n",
                "                if elapsed < time_window:\n",
                "                    await asyncio.sleep(time_window - elapsed)\n",
                "\n",
                "        print(f\"[Fail] '{item}' max retries exceeded.\")\n",
                "        return None\n",
                "\n",
                "    tasks = {item: asyncio.create_task(process_item(item)) for item in items}\n",
                "    output = []\n",
                "    for item in tqdm(items, desc=\"Processing\"):\n",
                "        try: \n",
                "            result = await process_item(item)\n",
                "            if result:\n",
                "                output.append({\n",
                "                    \"item\": item,\n",
                "                    \"response\": result.model_dump() if hasattr(result, \"model_dump\") else result\n",
                "                })\n",
                "        except Exception as e:\n",
                "            print(f\"[Unhandled Error] '{item}': {e}\")\n",
                "\n",
                "    return output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class FilmRecommendation(BaseModel):\n",
                "    title: str\n",
                "    blurb: str\n",
                "    review: str\n",
                "\n",
                "res = await async_prompts(\n",
                "    model='gpt-4o',\n",
                "    context=\"\"\"You are an eager member of staff at the last surviving Blockbuster store.\n",
                "    Recommend a film to the user based on their stated preferences.\n",
                "    You should give them the film title, the blurb in a sentence and then a quick one \n",
                "    sentence long review to hype them up for it.\"\"\",\n",
                "    items = [\"  a psychological horror film that will actually unsettle me\",\n",
                "                \"  something very romantic, Y2K, nostalgic\",\n",
                "                \"  strange, obscure and quite old\",\n",
                "                \"  a film where I don't have to pay attention and can doomscroll on my phone\"],\n",
                "    prompt_template = \"User preference: {item}\",\n",
                "    response_format=FilmRecommendation,\n",
                "    cache_dir='./tmp/llm_cache'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "res"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "show_doc(adulib.llm.prompt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "__client = None\n",
                "\n",
                "def prompt(model,\n",
                "                prompt,\n",
                "                context=\"You are a helpful assistant.\",\n",
                "                api_key=None,\n",
                "                response_format=None,\n",
                "                use_cache=True,\n",
                "                cache_dir=None,\n",
                "                include_model_in_cache_key=False,\n",
                "                cache_key_prepend=''):\n",
                "    global __client, __llm_cache\n",
                "    if __client is None:\n",
                "        if api_key is None: api_key = os.environ.get(\"PROJ_OPENAI_API_KEY\")\n",
                "        __client = OpenAI(api_key=api_key)\n",
                "        \n",
                "    response_schema = str(response_format.model_json_schema()) if response_format else \"\"\n",
                "        \n",
                "    if __llm_cache is None:\n",
                "        if cache_dir is None: cache_dir = tempfile.mkdtemp()\n",
                "        abs_cache_path = Path(cache_dir).resolve().as_posix()\n",
                "        __llm_cache = diskcache.Cache(abs_cache_path, eviction_policy=\"none\", size_limit=2**40)\n",
                "    \n",
                "    _model_key = model if include_model_in_cache_key else '*'\n",
                "    cache_key = f'{cache_key_prepend}:{_model_key}:{prompt}:{context}:{response_schema}'\n",
                "    \n",
                "    if use_cache and cache_key in __llm_cache:\n",
                "        output =  __llm_cache[cache_key]\n",
                "    else:\n",
                "        if not response_schema:\n",
                "            chat_completion = __client.chat.completions.create(  # Changed to synchronous call\n",
                "                messages=[\n",
                "                    {\n",
                "                        \"role\": \"system\",\n",
                "                        \"content\": context\n",
                "                    },\n",
                "                    {\n",
                "                        \"role\": \"user\",\n",
                "                        \"content\": prompt,\n",
                "                    }\n",
                "                ],\n",
                "                model=model,\n",
                "            )\n",
                "        else:\n",
                "            chat_completion = __client.beta.chat.completions.parse(  # Changed to synchronous call\n",
                "                messages=[\n",
                "                    {\n",
                "                        \"role\": \"system\",\n",
                "                        \"content\": context\n",
                "                    },\n",
                "                    {\n",
                "                        \"role\": \"user\",\n",
                "                        \"content\": prompt,\n",
                "                    }\n",
                "                ],\n",
                "                model=model,\n",
                "                response_format=response_format\n",
                "            )\n",
                "            \n",
                "        output = chat_completion.choices[0].message.content\n",
                "        __llm_cache[cache_key] = output\n",
                "    \n",
                "    if response_format:\n",
                "        return response_format(**json.loads(output))\n",
                "    else:\n",
                "        return output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prompt(\n",
                "    model='gpt-4o',\n",
                "    context='You are a helpful assistant.',\n",
                "    prompt='Hello, how are you?',\n",
                "    cache_dir='./tmp/llm_cache'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {},
    "nbformat": 4,
    "nbformat_minor": 4
}
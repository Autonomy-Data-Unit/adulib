{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# adulib.llm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|default_exp llm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "import nblite; from nbdev.showdoc import show_doc; nblite.nbl_export()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "try:\n",
                "    import os\n",
                "    import json\n",
                "    import tempfile\n",
                "    from pathlib import Path\n",
                "    from openai import OpenAI, AsyncOpenAI\n",
                "    import tiktoken\n",
                "    from asynciolimiter import Limiter\n",
                "    import diskcache\n",
                "except ImportError as e:\n",
                "    raise ImportError(f\"Install adulib[{__name__.split('.')[-1]}] to use this API.\") from e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import adulib.llm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "model_context_windows = {\n",
                "    'gpt-4o': 128000 * 0.8,\n",
                "    'gpt-4o-mini': 128000 * 0.8,\n",
                "}\n",
                "\n",
                "model_rate_limits = {\n",
                "    'gpt-4o': 10000,\n",
                "    'gpt-4o-mini': 30000,\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "---\n",
                            "\n",
                            "### tokenize_text\n",
                            "\n",
                            ">      tokenize_text (text, llm_model)"
                        ],
                        "text/plain": [
                            "---\n",
                            "\n",
                            "### tokenize_text\n",
                            "\n",
                            ">      tokenize_text (text, llm_model)"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "show_doc(adulib.llm.tokenize_text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "__model_tokenisers = {}\n",
                "\n",
                "def tokenize_text(text, llm_model):\n",
                "    if llm_model not in __model_tokenisers:\n",
                "        __model_tokenisers[llm_model] = tiktoken.encoding_for_model(llm_model)\n",
                "    return __model_tokenisers[llm_model].encode(text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[24912, 2375]"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "tokenize_text(\"hello world\", 'gpt-4o')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "---\n",
                            "\n",
                            "### detokenize_text\n",
                            "\n",
                            ">      detokenize_text (tokens, llm_model)"
                        ],
                        "text/plain": [
                            "---\n",
                            "\n",
                            "### detokenize_text\n",
                            "\n",
                            ">      detokenize_text (tokens, llm_model)"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "show_doc(adulib.llm.detokenize_text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "def detokenize_text(tokens, llm_model):\n",
                "    if llm_model not in __model_tokenisers:\n",
                "        __model_tokenisers[llm_model] = tiktoken.encoding_for_model(llm_model)\n",
                "    tokeniser_enc = tiktoken.encoding_for_model(llm_model)\n",
                "    return tokeniser_enc.decode(tokens)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'hello world'"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "llm_model = 'gpt-4o'\n",
                "detokenize_text(tokenize_text(\"hello world\", llm_model), llm_model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "---\n",
                            "\n",
                            "### get_token_count\n",
                            "\n",
                            ">      get_token_count (text, llm_model)"
                        ],
                        "text/plain": [
                            "---\n",
                            "\n",
                            "### get_token_count\n",
                            "\n",
                            ">      get_token_count (text, llm_model)"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "show_doc(adulib.llm.get_token_count)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "def get_token_count(text, llm_model):\n",
                "    return len(tokenize_text(text, llm_model))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "2"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "get_token_count(\"hello world\", \"gpt-4o\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "---\n",
                            "\n",
                            "### async_prompt\n",
                            "\n",
                            ">      async_prompt (model, prompt, context='You are a helpful assistant.',\n",
                            ">                    api_key=None, response_format=None, use_cache=True,\n",
                            ">                    cache_dir=None, include_model_in_cache_key=False,\n",
                            ">                    cache_key_prepend='')"
                        ],
                        "text/plain": [
                            "---\n",
                            "\n",
                            "### async_prompt\n",
                            "\n",
                            ">      async_prompt (model, prompt, context='You are a helpful assistant.',\n",
                            ">                    api_key=None, response_format=None, use_cache=True,\n",
                            ">                    cache_dir=None, include_model_in_cache_key=False,\n",
                            ">                    cache_key_prepend='')"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "show_doc(adulib.llm.async_prompt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "__client = None\n",
                "__model_rate_limiters = {}\n",
                "__llm_cache = None\n",
                "\n",
                "async def async_prompt(model,\n",
                "                           prompt,\n",
                "                           context=\"You are a helpful assistant.\",\n",
                "                           api_key=None,\n",
                "                           response_format=None,\n",
                "                           use_cache=True,\n",
                "                           cache_dir=None,\n",
                "                           include_model_in_cache_key=False,\n",
                "                           cache_key_prepend=''):\n",
                "    global __client, __llm_cache\n",
                "    if __client is None:\n",
                "        if api_key is None: api_key = os.environ.get(\"PROJ_OPENAI_API_KEY\")\n",
                "        __client = AsyncOpenAI(api_key=api_key)\n",
                "        \n",
                "    if model not in __model_rate_limiters:\n",
                "        __model_rate_limiters[model] = Limiter(model_rate_limits[model]/60)\n",
                "        \n",
                "    response_schema = str(response_format.model_json_schema()) if response_format else \"\"\n",
                "        \n",
                "    if __llm_cache is None:\n",
                "        if cache_dir is None: cache_dir = tempfile.mkdtemp()\n",
                "        abs_cache_path = Path(cache_dir).resolve().as_posix()\n",
                "        __llm_cache = diskcache.Cache(abs_cache_path, eviction_policy=\"none\", size_limit=2**40)\n",
                "    \n",
                "    _model_key = model if include_model_in_cache_key else '*'\n",
                "    cache_key = f'{cache_key_prepend}:{_model_key}:{prompt}:{context}:{response_schema}'\n",
                "    \n",
                "    if use_cache and cache_key in __llm_cache:\n",
                "        output =  __llm_cache[cache_key]\n",
                "    else:\n",
                "        await __model_rate_limiters[model].wait()\n",
                "        if not response_schema:\n",
                "            chat_completion = await __client.chat.completions.create(\n",
                "                messages=[\n",
                "                    {\n",
                "                        \"role\": \"system\",\n",
                "                        \"content\": context\n",
                "                    },\n",
                "                    {\n",
                "                        \"role\": \"user\",\n",
                "                        \"content\": prompt,\n",
                "                    }\n",
                "                ],\n",
                "                model=model,\n",
                "            )\n",
                "        else:\n",
                "            chat_completion = await __client.beta.chat.completions.parse(\n",
                "                messages=[\n",
                "                    {\n",
                "                        \"role\": \"system\",\n",
                "                        \"content\": context\n",
                "                    },\n",
                "                    {\n",
                "                        \"role\": \"user\",\n",
                "                        \"content\": prompt,\n",
                "                    }\n",
                "                ],\n",
                "                model=model,\n",
                "                response_format=response_format\n",
                "            )\n",
                "            \n",
                "        output = chat_completion.choices[0].message.content\n",
                "        __llm_cache[cache_key] = output\n",
                "    \n",
                "    if response_format:\n",
                "        return response_format(**json.loads(output))\n",
                "    else:\n",
                "        return output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\""
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "await async_prompt(\n",
                "    model='gpt-4o',\n",
                "    context='You are a helpful assistant.',\n",
                "    prompt='Hello, how are you?',\n",
                "    cache_dir='./tmp/llm_cache'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pydantic import BaseModel\n",
                "from pprint import pprint"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'final_answer': 'x = -15/4 or x = -3.75',\n",
                        " 'steps': [{'explanation': 'The given equation is a linear equation in one '\n",
                        "                           'variable, where we need to isolate the variable x '\n",
                        "                           'to find its value. The equation is currently 8x + '\n",
                        "                           '7 = -23.',\n",
                        "            'output': 'Understand the objective: isolate x.'},\n",
                        "           {'explanation': 'Start by eliminating the constant term on the left '\n",
                        "                           'side of the equation, which is +7. We do this by '\n",
                        "                           'subtracting 7 from both sides of the equation.',\n",
                        "            'output': 'Subtract 7 from both sides: 8x + 7 - 7 = -23 - 7'},\n",
                        "           {'explanation': 'Simplify both sides of the equation by removing '\n",
                        "                           'the +7 and -7, since they cancel each other out on '\n",
                        "                           'the left side.',\n",
                        "            'output': 'Simplified to: 8x = -30'},\n",
                        "           {'explanation': 'Now, we have 8 times x equals -30. Our goal is to '\n",
                        "                           'solve for x, so divide both sides by 8 to isolate '\n",
                        "                           'x.',\n",
                        "            'output': 'Divide both sides by 8: (8x) / 8 = (-30) / 8'},\n",
                        "           {'explanation': 'Simplifying the division on both sides gives the '\n",
                        "                           'solution for x. The fraction -30/8 can be further '\n",
                        "                           'simplified by dividing both the numerator and the '\n",
                        "                           'denominator by their greatest common divisor, '\n",
                        "                           'which is 2.',\n",
                        "            'output': 'Simplify to: x = -15/4 or x = -3.75'}]}\n"
                    ]
                }
            ],
            "source": [
                "class Step(BaseModel):\n",
                "    explanation: str\n",
                "    output: str\n",
                "\n",
                "class MathReasoning(BaseModel):\n",
                "    steps: list[Step]\n",
                "    final_answer: str\n",
                "    \n",
                "res = await async_prompt(\n",
                "    model='gpt-4o',\n",
                "    context='You are a helpful math tutor. Guide the user through the solution step by step.',\n",
                "    prompt='How can I solve 8x + 7 = -23?',\n",
                "    response_format=MathReasoning,\n",
                "    cache_dir='./tmp/llm_cache'\n",
                ")\n",
                "\n",
                "pprint(res.model_dump())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "---\n",
                            "\n",
                            "### prompt\n",
                            "\n",
                            ">      prompt (model, prompt, context='You are a helpful assistant.',\n",
                            ">              api_key=None, response_format=None, use_cache=True,\n",
                            ">              cache_dir=None, include_model_in_cache_key=False,\n",
                            ">              cache_key_prepend='')"
                        ],
                        "text/plain": [
                            "---\n",
                            "\n",
                            "### prompt\n",
                            "\n",
                            ">      prompt (model, prompt, context='You are a helpful assistant.',\n",
                            ">              api_key=None, response_format=None, use_cache=True,\n",
                            ">              cache_dir=None, include_model_in_cache_key=False,\n",
                            ">              cache_key_prepend='')"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "show_doc(adulib.llm.prompt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "__client = None\n",
                "\n",
                "def prompt(model,\n",
                "                prompt,\n",
                "                context=\"You are a helpful assistant.\",\n",
                "                api_key=None,\n",
                "                response_format=None,\n",
                "                use_cache=True,\n",
                "                cache_dir=None,\n",
                "                include_model_in_cache_key=False,\n",
                "                cache_key_prepend=''):\n",
                "    global __client, __llm_cache\n",
                "    if __client is None:\n",
                "        if api_key is None: api_key = os.environ.get(\"PROJ_OPENAI_API_KEY\")\n",
                "        __client = OpenAI(api_key=api_key)\n",
                "        \n",
                "    response_schema = str(response_format.model_json_schema()) if response_format else \"\"\n",
                "        \n",
                "    if __llm_cache is None:\n",
                "        if cache_dir is None: cache_dir = tempfile.mkdtemp()\n",
                "        abs_cache_path = Path(cache_dir).resolve().as_posix()\n",
                "        __llm_cache = diskcache.Cache(abs_cache_path, eviction_policy=\"none\", size_limit=2**40)\n",
                "    \n",
                "    _model_key = model if include_model_in_cache_key else '*'\n",
                "    cache_key = f'{cache_key_prepend}:{_model_key}:{prompt}:{context}:{response_schema}'\n",
                "    \n",
                "    if use_cache and cache_key in __llm_cache:\n",
                "        output =  __llm_cache[cache_key]\n",
                "    else:\n",
                "        if not response_schema:\n",
                "            chat_completion = __client.chat.completions.create(  # Changed to synchronous call\n",
                "                messages=[\n",
                "                    {\n",
                "                        \"role\": \"system\",\n",
                "                        \"content\": context\n",
                "                    },\n",
                "                    {\n",
                "                        \"role\": \"user\",\n",
                "                        \"content\": prompt,\n",
                "                    }\n",
                "                ],\n",
                "                model=model,\n",
                "            )\n",
                "        else:\n",
                "            chat_completion = __client.beta.chat.completions.parse(  # Changed to synchronous call\n",
                "                messages=[\n",
                "                    {\n",
                "                        \"role\": \"system\",\n",
                "                        \"content\": context\n",
                "                    },\n",
                "                    {\n",
                "                        \"role\": \"user\",\n",
                "                        \"content\": prompt,\n",
                "                    }\n",
                "                ],\n",
                "                model=model,\n",
                "                response_format=response_format\n",
                "            )\n",
                "            \n",
                "        output = chat_completion.choices[0].message.content\n",
                "        __llm_cache[cache_key] = output\n",
                "    \n",
                "    if response_format:\n",
                "        return response_format(**json.loads(output))\n",
                "    else:\n",
                "        return output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\""
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "prompt(\n",
                "    model='gpt-4o',\n",
                "    context='You are a helpful assistant.',\n",
                "    prompt='Hello, how are you?',\n",
                "    cache_dir='./tmp/llm_cache'\n",
                ")"
            ]
        }
    ],
    "metadata": {},
    "nbformat": 4,
    "nbformat_minor": 2
}
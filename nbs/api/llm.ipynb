{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# adulib.llm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|default_exp llm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "import nblite; from nbdev.showdoc import show_doc; nblite.nbl_export()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "try:\n",
                "    import os\n",
                "    import json\n",
                "    import tempfile\n",
                "    from pathlib import Path\n",
                "    from openai import OpenAI, AsyncOpenAI\n",
                "    import tiktoken\n",
                "    from asynciolimiter import Limiter\n",
                "    import diskcache\n",
                "    from typing import List, Optional, Type, Dict, Any\n",
                "    import asyncio\n",
                "    import time\n",
                "    from tqdm import tqdm\n",
                "    from pydantic import BaseModel\n",
                "except ImportError as e:\n",
                "    raise ImportError(f\"Install adulib[{__name__.split('.')[-1]}] to use this API.\") from e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from dotenv import load_dotenv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import adulib.llm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "load_dotenv()\n",
                "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "model_context_windows = {\n",
                "    'gpt-4o': 128000 * 0.8,\n",
                "    'gpt-4o-mini': 128000 * 0.8,\n",
                "}\n",
                "\n",
                "model_rate_limits = {\n",
                "    'gpt-4o': 10000,\n",
                "    'gpt-4o-mini': 30000,\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "---\n",
                            "\n",
                            "### tokenize_text\n",
                            "\n",
                            ">      tokenize_text (text, llm_model)"
                        ],
                        "text/plain": [
                            "---\n",
                            "\n",
                            "### tokenize_text\n",
                            "\n",
                            ">      tokenize_text (text, llm_model)"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "show_doc(adulib.llm.tokenize_text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "__model_tokenisers = {}\n",
                "\n",
                "def tokenize_text(text, llm_model):\n",
                "    if llm_model not in __model_tokenisers:\n",
                "        __model_tokenisers[llm_model] = tiktoken.encoding_for_model(llm_model)\n",
                "    return __model_tokenisers[llm_model].encode(text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[24912, 2375]"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "tokenize_text(\"hello world\", 'gpt-4o')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "---\n",
                            "\n",
                            "### detokenize_text\n",
                            "\n",
                            ">      detokenize_text (tokens, llm_model)"
                        ],
                        "text/plain": [
                            "---\n",
                            "\n",
                            "### detokenize_text\n",
                            "\n",
                            ">      detokenize_text (tokens, llm_model)"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "show_doc(adulib.llm.detokenize_text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "def detokenize_text(tokens, llm_model):\n",
                "    if llm_model not in __model_tokenisers:\n",
                "        __model_tokenisers[llm_model] = tiktoken.encoding_for_model(llm_model)\n",
                "    tokeniser_enc = tiktoken.encoding_for_model(llm_model)\n",
                "    return tokeniser_enc.decode(tokens)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'hello world'"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "llm_model = 'gpt-4o'\n",
                "detokenize_text(tokenize_text(\"hello world\", llm_model), llm_model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "---\n",
                            "\n",
                            "### get_token_count\n",
                            "\n",
                            ">      get_token_count (text, llm_model)"
                        ],
                        "text/plain": [
                            "---\n",
                            "\n",
                            "### get_token_count\n",
                            "\n",
                            ">      get_token_count (text, llm_model)"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "show_doc(adulib.llm.get_token_count)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "def get_token_count(text, llm_model):\n",
                "    return len(tokenize_text(text, llm_model))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "2"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "get_token_count(\"hello world\", \"gpt-4o\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "---\n",
                            "\n",
                            "### async_prompt\n",
                            "\n",
                            ">      async_prompt (model, prompt, context='You are a helpful assistant.',\n",
                            ">                    api_key=None, response_format=None, use_cache=True,\n",
                            ">                    cache_dir=None, include_model_in_cache_key=False,\n",
                            ">                    cache_key_prepend='')"
                        ],
                        "text/plain": [
                            "---\n",
                            "\n",
                            "### async_prompt\n",
                            "\n",
                            ">      async_prompt (model, prompt, context='You are a helpful assistant.',\n",
                            ">                    api_key=None, response_format=None, use_cache=True,\n",
                            ">                    cache_dir=None, include_model_in_cache_key=False,\n",
                            ">                    cache_key_prepend='')"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "show_doc(adulib.llm.async_prompt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "__client = None\n",
                "__model_rate_limiters = {}\n",
                "__llm_cache = None\n",
                "\n",
                "async def async_prompt(model,\n",
                "                           prompt,\n",
                "                           context=\"You are a helpful assistant.\",\n",
                "                           api_key=None,\n",
                "                           response_format=None,\n",
                "                           use_cache=True,\n",
                "                           cache_dir=None,\n",
                "                           include_model_in_cache_key=False,\n",
                "                           cache_key_prepend=''):\n",
                "    global __client, __llm_cache\n",
                "    if __client is None:\n",
                "        if api_key is None: api_key = os.environ.get(\"PROJ_OPENAI_API_KEY\")\n",
                "        __client = AsyncOpenAI(api_key=api_key)\n",
                "        \n",
                "    if model not in __model_rate_limiters:\n",
                "        __model_rate_limiters[model] = Limiter(model_rate_limits[model]/60)\n",
                "        \n",
                "    response_schema = str(response_format.model_json_schema()) if response_format else \"\"\n",
                "        \n",
                "    if __llm_cache is None:\n",
                "        if cache_dir is None: cache_dir = tempfile.mkdtemp()\n",
                "        abs_cache_path = Path(cache_dir).resolve().as_posix()\n",
                "        __llm_cache = diskcache.Cache(abs_cache_path, eviction_policy=\"none\", size_limit=2**40)\n",
                "    \n",
                "    _model_key = model if include_model_in_cache_key else '*'\n",
                "    cache_key = f'{cache_key_prepend}:{_model_key}:{prompt}:{context}:{response_schema}'\n",
                "    \n",
                "    if use_cache and cache_key in __llm_cache:\n",
                "        output =  __llm_cache[cache_key]\n",
                "    else:\n",
                "        await __model_rate_limiters[model].wait()\n",
                "        if not response_schema:\n",
                "            chat_completion = await __client.chat.completions.create(\n",
                "                messages=[\n",
                "                    {\n",
                "                        \"role\": \"system\",\n",
                "                        \"content\": context\n",
                "                    },\n",
                "                    {\n",
                "                        \"role\": \"user\",\n",
                "                        \"content\": prompt,\n",
                "                    }\n",
                "                ],\n",
                "                model=model,\n",
                "            )\n",
                "        else:\n",
                "            chat_completion = await __client.beta.chat.completions.parse(\n",
                "                messages=[\n",
                "                    {\n",
                "                        \"role\": \"system\",\n",
                "                        \"content\": context\n",
                "                    },\n",
                "                    {\n",
                "                        \"role\": \"user\",\n",
                "                        \"content\": prompt,\n",
                "                    }\n",
                "                ],\n",
                "                model=model,\n",
                "                response_format=response_format\n",
                "            )\n",
                "        output = chat_completion.choices[0].message.content\n",
                "        __llm_cache[cache_key] = output\n",
                "    \n",
                "    if response_format:\n",
                "        return response_format(**json.loads(output))\n",
                "    else:\n",
                "        return output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need. How can I assist you today?\""
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "await async_prompt(\n",
                "    model='gpt-4o',\n",
                "    context='You are a helpful assistant.',\n",
                "    prompt='Hello, how are you?',\n",
                "    cache_dir='./tmp/llm_cache'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pydantic import BaseModel\n",
                "from pprint import pprint"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'final_answer': 'x = -\\\\frac{15}{4}',\n",
                        " 'steps': [{'explanation': 'Subtract 7 from both sides to isolate the term '\n",
                        "                           'with x.',\n",
                        "            'output': '8x + 7 - 7 = -23 - 7'},\n",
                        "           {'explanation': 'Simplify both sides: 7 - 7 is 0 on the left, and '\n",
                        "                           'we calculate -23 - 7 on the right.',\n",
                        "            'output': '8x = -30'},\n",
                        "           {'explanation': 'Divide both sides by 8 to solve for x.',\n",
                        "            'output': 'x = -30 / 8'},\n",
                        "           {'explanation': 'Simplify the fraction by dividing both the '\n",
                        "                           'numerator and the denominator by 2.',\n",
                        "            'output': 'x = -15 / 4'}]}\n"
                    ]
                }
            ],
            "source": [
                "class Step(BaseModel):\n",
                "    explanation: str\n",
                "    output: str\n",
                "\n",
                "class MathReasoning(BaseModel):\n",
                "    steps: list[Step]\n",
                "    final_answer: str\n",
                "    \n",
                "res = await async_prompt(\n",
                "    model='gpt-4o',\n",
                "    context='You are a helpful math tutor. Guide the user through the solution step by step.',\n",
                "    prompt='How can I solve 8x + 7 = -23?',\n",
                "    response_format=MathReasoning,\n",
                "    cache_dir='./tmp/llm_cache'\n",
                ")\n",
                "\n",
                "pprint(res.model_dump())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "---\n",
                            "\n",
                            "### async_prompts\n",
                            "\n",
                            ">      async_prompts (model:str, context:str, items:List[str],\n",
                            ">                     prompt_template:str, response_format:Optional[Type[pydanti\n",
                            ">                     c.main.BaseModel]]=None, api_key:Optional[str]=None,\n",
                            ">                     cache_dir:Optional[str]=None,\n",
                            ">                     include_model_in_cache_key:bool=False,\n",
                            ">                     cache_key_prepend:str='', max_retries:int=3,\n",
                            ">                     timeout:int=30, time_window=20,\n",
                            ">                     concurrency_limit:Optional[int]=None)"
                        ],
                        "text/plain": [
                            "---\n",
                            "\n",
                            "### async_prompts\n",
                            "\n",
                            ">      async_prompts (model:str, context:str, items:List[str],\n",
                            ">                     prompt_template:str, response_format:Optional[Type[pydanti\n",
                            ">                     c.main.BaseModel]]=None, api_key:Optional[str]=None,\n",
                            ">                     cache_dir:Optional[str]=None,\n",
                            ">                     include_model_in_cache_key:bool=False,\n",
                            ">                     cache_key_prepend:str='', max_retries:int=3,\n",
                            ">                     timeout:int=30, time_window=20,\n",
                            ">                     concurrency_limit:Optional[int]=None)"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "show_doc(adulib.llm.async_prompts)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "async def async_prompts(\n",
                "    model: str,\n",
                "    context: str,\n",
                "    items: List[str],\n",
                "    prompt_template: str,\n",
                "    response_format: Optional[Type[BaseModel]] = None,\n",
                "    api_key: Optional[str] = None,\n",
                "    cache_dir: Optional[str] = None,\n",
                "    include_model_in_cache_key: bool = False,\n",
                "    cache_key_prepend: str = '',\n",
                "    max_retries: int = 3,\n",
                "    timeout: int = 30,\n",
                "    time_window = 20,\n",
                "    concurrency_limit: Optional[int] = None\n",
                "    ):\n",
                "    semaphore = asyncio.Semaphore(concurrency_limit) if concurrency_limit else asyncio.Semaphore(len(items))\n",
                "    \n",
                "    async def process_item(item: str):\n",
                "        prompt = prompt_template.format(item=item)\n",
                "\n",
                "        for attempt in range(max_retries):\n",
                "            async with semaphore:\n",
                "                start_time = time.time()\n",
                "                try:\n",
                "                    task = asyncio.create_task(\n",
                "                        async_prompt(\n",
                "                            model=model,\n",
                "                            prompt=prompt,\n",
                "                            context=context,\n",
                "                            response_format=response_format,\n",
                "                            api_key=api_key,\n",
                "                            use_cache=True,\n",
                "                            cache_dir=cache_dir,\n",
                "                            include_model_in_cache_key=include_model_in_cache_key,\n",
                "                            cache_key_prepend=cache_key_prepend\n",
                "                        )\n",
                "                    )\n",
                "\n",
                "                    done, pending = await asyncio.wait({task}, timeout=timeout)\n",
                "\n",
                "                    if task in done:\n",
                "                        return task.result()\n",
                "\n",
                "                    else:\n",
                "                        task.cancel()\n",
                "                        print(f\"[Timeout] '{item}' attempt {attempt + 1}/{max_retries} exceeded {timeout}s\")\n",
                "\n",
                "                except asyncio.CancelledError:\n",
                "                    print(f\"[Cancelled] '{item}' attempt {attempt + 1}/{max_retries}\")\n",
                "\n",
                "                except Exception as e:\n",
                "                    print(f\"[Error] '{item}' attempt {attempt + 1}/{max_retries}: {e}\")\n",
                "\n",
                "                await asyncio.sleep(2 ** attempt)  # exponential backoff\n",
                "\n",
                "                elapsed = time.time() - start_time\n",
                "                if elapsed < time_window:\n",
                "                    await asyncio.sleep(time_window - elapsed)\n",
                "\n",
                "        print(f\"[Fail] '{item}' max retries exceeded.\")\n",
                "        return None\n",
                "\n",
                "    tasks = {item: asyncio.create_task(process_item(item)) for item in items}\n",
                "    output = []\n",
                "    for item in tqdm(items, desc=\"Processing\"):\n",
                "        try: \n",
                "            result = await process_item(item)\n",
                "            if result:\n",
                "                output.append({\n",
                "                    \"item\": item,\n",
                "                    \"response\": result.model_dump() if hasattr(result, \"model_dump\") else result\n",
                "                })\n",
                "        except Exception as e:\n",
                "            print(f\"[Unhandled Error] '{item}': {e}\")\n",
                "\n",
                "    return output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Processing: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 1160.17it/s]\n"
                    ]
                }
            ],
            "source": [
                "class FilmRecommendation(BaseModel):\n",
                "    title: str\n",
                "    blurb: str\n",
                "    review: str\n",
                "\n",
                "res = await async_prompts(\n",
                "    model='gpt-4o',\n",
                "    context=\"\"\"You are an eager member of staff at the last surviving Blockbuster store.\n",
                "    Recommend a film to the user based on their stated preferences.\n",
                "    You should give them the film title, the blurb in a sentence and then a quick one \n",
                "    sentence long review to hype them up for it.\"\"\",\n",
                "    items = [\"  a psychological horror film that will actually unsettle me\",\n",
                "                \"  something very romantic, Y2K, nostalgic\",\n",
                "                \"  strange, obscure and quite old\",\n",
                "                \"  a film where I don't have to pay attention and can doomscroll on my phone\"],\n",
                "    prompt_template = \"User preference: {item}\",\n",
                "    response_format=FilmRecommendation,\n",
                "    cache_dir='./tmp/llm_cache'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[{'item': '  a psychological horror film that will actually unsettle me',\n",
                            "  'response': {'title': 'Hereditary',\n",
                            "   'blurb': 'When a family begins to unravel after the death of their secretive grandmother, they uncover terrifying truths about their ancestry that will test their sanity.',\n",
                            "   'review': 'This chilling masterpiece leaves you in a state of dread long after the credits roll, with haunting performances and an unforgettable atmosphere that digs deep under your skin.'}},\n",
                            " {'item': '  something very romantic, Y2K, nostalgic',\n",
                            "  'response': {'title': 'Serendipity',\n",
                            "   'blurb': 'A romantic tale of fate and destiny, where two strangers, Jonathan and Sara, impulsively explore a connection over Christmas in New York City, only to leave their future up in the air to chance.',\n",
                            "   'review': 'This movie will sweep you off your feet with its enchanting story of love and destiny, leaving you with a warm nostalgia for the early 2000s romance genre.'}},\n",
                            " {'item': '  strange, obscure and quite old',\n",
                            "  'response': {'title': 'Eraserhead',\n",
                            "   'blurb': 'In David Lynch\u2019s surreal and disturbing debut film, Henry Spencer tries to survive his industrial environment, his angry girlfriend, and the unbearable screams of his newly born mutant child.',\n",
                            "   'review': 'Eraserhead is an atmospheric and haunting classic that dives deep into the nightmares of the subconscious, solidifying Lynch as a master of the surreal and bizarre.'}},\n",
                            " {'item': \"  a film where I don't have to pay attention and can doomscroll on my phone\",\n",
                            "  'response': {'title': 'Zoolander',\n",
                            "   'blurb': 'The world of male modeling gets turned upside down when a clueless but charming top model is brainwashed to assassinate the Prime Minister of Malaysia.',\n",
                            "   'review': \"With its ridiculous humor and over-the-top antics, 'Zoolander' is the perfect distraction-comedy to half-watch and still find yourself laughing out loud.\"}}]"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "res"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "---\n",
                            "\n",
                            "### prompt\n",
                            "\n",
                            ">      prompt (model, prompt, context='You are a helpful assistant.',\n",
                            ">              api_key=None, response_format=None, use_cache=True,\n",
                            ">              cache_dir=None, include_model_in_cache_key=False,\n",
                            ">              cache_key_prepend='')"
                        ],
                        "text/plain": [
                            "---\n",
                            "\n",
                            "### prompt\n",
                            "\n",
                            ">      prompt (model, prompt, context='You are a helpful assistant.',\n",
                            ">              api_key=None, response_format=None, use_cache=True,\n",
                            ">              cache_dir=None, include_model_in_cache_key=False,\n",
                            ">              cache_key_prepend='')"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "show_doc(adulib.llm.prompt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "__client = None\n",
                "\n",
                "def prompt(model,\n",
                "                prompt,\n",
                "                context=\"You are a helpful assistant.\",\n",
                "                api_key=None,\n",
                "                response_format=None,\n",
                "                use_cache=True,\n",
                "                cache_dir=None,\n",
                "                include_model_in_cache_key=False,\n",
                "                cache_key_prepend=''):\n",
                "    global __client, __llm_cache\n",
                "    if __client is None:\n",
                "        if api_key is None: api_key = os.environ.get(\"PROJ_OPENAI_API_KEY\")\n",
                "        __client = OpenAI(api_key=api_key)\n",
                "        \n",
                "    response_schema = str(response_format.model_json_schema()) if response_format else \"\"\n",
                "        \n",
                "    if __llm_cache is None:\n",
                "        if cache_dir is None: cache_dir = tempfile.mkdtemp()\n",
                "        abs_cache_path = Path(cache_dir).resolve().as_posix()\n",
                "        __llm_cache = diskcache.Cache(abs_cache_path, eviction_policy=\"none\", size_limit=2**40)\n",
                "    \n",
                "    _model_key = model if include_model_in_cache_key else '*'\n",
                "    cache_key = f'{cache_key_prepend}:{_model_key}:{prompt}:{context}:{response_schema}'\n",
                "    \n",
                "    if use_cache and cache_key in __llm_cache:\n",
                "        output =  __llm_cache[cache_key]\n",
                "    else:\n",
                "        if not response_schema:\n",
                "            chat_completion = __client.chat.completions.create(  # Changed to synchronous call\n",
                "                messages=[\n",
                "                    {\n",
                "                        \"role\": \"system\",\n",
                "                        \"content\": context\n",
                "                    },\n",
                "                    {\n",
                "                        \"role\": \"user\",\n",
                "                        \"content\": prompt,\n",
                "                    }\n",
                "                ],\n",
                "                model=model,\n",
                "            )\n",
                "        else:\n",
                "            chat_completion = __client.beta.chat.completions.parse(  # Changed to synchronous call\n",
                "                messages=[\n",
                "                    {\n",
                "                        \"role\": \"system\",\n",
                "                        \"content\": context\n",
                "                    },\n",
                "                    {\n",
                "                        \"role\": \"user\",\n",
                "                        \"content\": prompt,\n",
                "                    }\n",
                "                ],\n",
                "                model=model,\n",
                "                response_format=response_format\n",
                "            )\n",
                "            \n",
                "        output = chat_completion.choices[0].message.content\n",
                "        __llm_cache[cache_key] = output\n",
                "    \n",
                "    if response_format:\n",
                "        return response_format(**json.loads(output))\n",
                "    else:\n",
                "        return output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need. How can I assist you today?\""
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "prompt(\n",
                "    model='gpt-4o',\n",
                "    context='You are a helpful assistant.',\n",
                "    prompt='Hello, how are you?',\n",
                "    cache_dir='./tmp/llm_cache'\n",
                ")"
            ]
        }
    ],
    "metadata": {},
    "nbformat": 4,
    "nbformat_minor": 4
}
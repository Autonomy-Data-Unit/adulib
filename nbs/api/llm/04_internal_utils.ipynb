{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "7adbfe75",
            "metadata": {},
            "source": [
                "# _utils"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ec4869ae",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|default_exp llm._utils"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "546ad7cd",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "import nblite; from nblite import show_doc; nblite.nbl_export()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f70577eb",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "try:\n",
                "    import litellm\n",
                "    import inspect\n",
                "    import time\n",
                "    import asyncio\n",
                "    from typing import Callable, Optional, Union\n",
                "    from pathlib import Path\n",
                "    from adulib.llm.caching import _cache_execute, _async_cache_execute, get_cache_key, is_in_cache\n",
                "    from adulib.llm.call_logging import _log_call\n",
                "    from adulib.llm.rate_limits import _get_limiter, default_retry_on_exception, default_max_retries, default_retry_delay, default_timeout\n",
                "except ImportError as e:\n",
                "    raise ImportError(f\"Install adulib[llm] to use this API.\") from e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dcacc75b",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "from adulib.caching import set_default_cache_path\n",
                "import adulib.llm._utils as this_module"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d0dbbb08",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "repo_path = nblite.config.get_project_root_and_config()[0]\n",
                "set_default_cache_path(repo_path / '.tmp_cache')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "00df9f82",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|exporti\n",
                "class MaximumRetriesException(Exception):\n",
                "    def __init__(self, retry_exceptions: list[Exception]):\n",
                "        self.retry_exceptions = retry_exceptions\n",
                "        self.retry_exceptions_str = \"\\n\".join([f\"{i}: ({type(e).__name__}) {e}\" for i, e in enumerate(retry_exceptions)])\n",
                "        super().__init__(f\"Maximum retries ({len(retry_exceptions)}) reached. Exceptions:\\n{self.retry_exceptions_str}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d0b7d3cb",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|exporti\n",
                "def _llm_func_factory(\n",
                "    func: Callable,\n",
                "    func_name: str,\n",
                "    func_cache_name: str,\n",
                "    retrieve_log_data: Optional[Callable] = None,\n",
                "):\n",
                "    func_sig = inspect.signature(func)\n",
                "    def llm_func(\n",
                "        *args,\n",
                "        # Cache settings\n",
                "        cache_enabled: bool=True,\n",
                "        cache_path: Optional[Union[str, Path]]=None,\n",
                "        cache_key_prefix: Optional[str]=None,\n",
                "        include_model_in_cache_key: bool=True,\n",
                "        return_cache_key: bool=False,\n",
                "        # Retry settings\n",
                "        enable_retries: bool=True,\n",
                "        retry_on_exceptions: Optional[list[Exception]]=None,\n",
                "        retry_on_all_exceptions: bool=False,\n",
                "        max_retries: Optional[int]=None,\n",
                "        retry_delay: Optional[int]=None,\n",
                "        **kwargs,\n",
                "    ):\n",
                "        if retry_on_exceptions is None: retry_on_exceptions = default_retry_on_exception\n",
                "        if max_retries is None: max_retries = default_max_retries\n",
                "        if retry_delay is None: retry_delay = default_retry_delay\n",
                "        \n",
                "        # Generate cache key\n",
                "        bound = func_sig.bind(*args, **kwargs)\n",
                "        func_args_and_kwargs = dict(bound.arguments)\n",
                "        model = func_args_and_kwargs.pop('model') # we treat 'model' separately, as we can optionally exclude it from the cache key\n",
                "        cache_key = get_cache_key(model, func_cache_name, func_args_and_kwargs, cache_key_prefix, include_model_in_cache_key)\n",
                "        if return_cache_key: return cache_key\n",
                "        \n",
                "        # Execute with caching and retries\n",
                "        success = False\n",
                "        exceptions = []\n",
                "        for _ in range(max_retries):\n",
                "            try:\n",
                "                retrieved_from_cache, result = _cache_execute(\n",
                "                    cache_key=cache_key,\n",
                "                    execute_func=lambda: func(*args, **kwargs),\n",
                "                    cache_enabled=cache_enabled,\n",
                "                    cache_path=cache_path,\n",
                "                )\n",
                "                success = True\n",
                "                break\n",
                "            except BaseException as e:\n",
                "                if not enable_retries: raise e\n",
                "                if not (retry_on_all_exceptions or any([isinstance(e, exc) for exc in retry_on_exceptions])): raise e\n",
                "                exceptions.append(e)\n",
                "                time.sleep(retry_delay)\n",
                "                    \n",
                "        if not success:\n",
                "            raise MaximumRetriesException(exceptions)\n",
                "        \n",
                "        # Call logging\n",
                "        if retrieve_log_data is not None:\n",
                "            if not retrieved_from_cache:\n",
                "                log_data = retrieve_log_data(model, func_args_and_kwargs, result)\n",
                "                _log_call( model=model, **log_data)\n",
                "        \n",
                "        return result\n",
                "    \n",
                "    llm_func.__name__ = func_name\n",
                "    return llm_func"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a2f85567",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Maximum retries (5) reached. Exceptions:\n",
                        "0: (RateLimitError) litellm.RateLimitError: None\n",
                        "1: (RateLimitError) litellm.RateLimitError: None\n",
                        "2: (RateLimitError) litellm.RateLimitError: None\n",
                        "3: (RateLimitError) litellm.RateLimitError: None\n",
                        "4: (RateLimitError) litellm.RateLimitError: None\n"
                    ]
                }
            ],
            "source": [
                "#|hide\n",
                "def foo(model):\n",
                "    raise litellm.RateLimitError(None, None, None)\n",
                "\n",
                "_foo = _llm_func_factory(\n",
                "    func=foo,\n",
                "    func_name=\"foo\",\n",
                "    func_cache_name=\"foo\",\n",
                ")\n",
                "\n",
                "try:\n",
                "    _foo(model=\"foo\", retry_delay=0.01)\n",
                "except MaximumRetriesException as e:\n",
                "    print(e)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8dd12b83",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Maximum retries (5) reached. Exceptions:\n",
                        "0: (ValueError) Failed!\n",
                        "1: (ValueError) Failed!\n",
                        "2: (ValueError) Failed!\n",
                        "3: (ValueError) Failed!\n",
                        "4: (ValueError) Failed!\n"
                    ]
                }
            ],
            "source": [
                "#|hide\n",
                "def foo(model):\n",
                "    raise ValueError(\"Failed!\")\n",
                "\n",
                "_foo = _llm_func_factory(\n",
                "    func=foo,\n",
                "    func_name=\"foo\",\n",
                "    func_cache_name=\"foo\",\n",
                "    retrieve_log_data=lambda model, func_kwargs, response: { \"method\": \"foo\", \"input_tokens\": None, \"output_tokens\": None, \"cost\": 0 },\n",
                ")\n",
                "\n",
                "try:\n",
                "    _foo(model=\"foo\", retry_on_exceptions=[ValueError], retry_delay=0.01)\n",
                "except MaximumRetriesException as e:\n",
                "    print(e)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d92cd0dd",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|exporti\n",
                "def _llm_async_func_factory(\n",
                "    func: Callable,\n",
                "    func_name: str,\n",
                "    func_cache_name: str,\n",
                "    retrieve_log_data: Optional[Callable] = None,\n",
                "):\n",
                "    func_sig = inspect.signature(func)\n",
                "    async def llm_func(\n",
                "        *args,\n",
                "        # Cache settings\n",
                "        cache_enabled: bool=True,\n",
                "        cache_path: Optional[Union[str, Path]]=None,\n",
                "        cache_key_prefix: Optional[str]=None,\n",
                "        include_model_in_cache_key: bool=True,\n",
                "        return_cache_key: bool=False,\n",
                "        # Retry settings\n",
                "        enable_retries: bool=True,\n",
                "        retry_on_exceptions: Optional[list[Exception]]=None,\n",
                "        retry_on_all_exceptions: bool=False,\n",
                "        max_retries: Optional[int]=None,\n",
                "        retry_delay: Optional[int]=None,\n",
                "        timeout: Optional[int]=None,\n",
                "        **kwargs,\n",
                "    ):\n",
                "        if retry_on_exceptions is None: retry_on_exceptions = default_retry_on_exception\n",
                "        if max_retries is None: max_retries = default_max_retries\n",
                "        if retry_delay is None: retry_delay = default_retry_delay\n",
                "        if timeout is None: timeout = default_timeout\n",
                "        \n",
                "        # Generate cache key\n",
                "        bound = func_sig.bind(*args, **kwargs)\n",
                "        func_args_and_kwargs = dict(bound.arguments)\n",
                "        model = func_args_and_kwargs.pop('model') # we treat 'model' separately, as we can optionally exclude it from the cache key\n",
                "        cache_key = get_cache_key(model, func_cache_name, func_args_and_kwargs, cache_key_prefix, include_model_in_cache_key)\n",
                "        if return_cache_key: return cache_key\n",
                "        \n",
                "        # Rate limiting\n",
                "        key_in_cache = is_in_cache(cache_key)\n",
                "        if not key_in_cache:\n",
                "            api_key = kwargs.get(\"api_key\", None)\n",
                "            await _get_limiter(model, api_key).wait()\n",
                "        \n",
                "        # Execute with caching and retries\n",
                "        success = False\n",
                "        exceptions = []\n",
                "        async def run_with_timeout():\n",
                "            return await asyncio.wait_for(func(*args, **kwargs), timeout)\n",
                "        for _ in range(max_retries):\n",
                "            try:                \n",
                "                retrieved_from_cache, result = await _async_cache_execute(\n",
                "                    cache_key=cache_key,\n",
                "                    execute_func=run_with_timeout if timeout is not None else lambda: func(*args, **kwargs),\n",
                "                    cache_enabled=cache_enabled,\n",
                "                    cache_path=cache_path,\n",
                "                )\n",
                "                success = True\n",
                "                break\n",
                "            except BaseException as e:\n",
                "                if not enable_retries: raise e\n",
                "                if not (retry_on_all_exceptions or any([isinstance(e, exc) for exc in retry_on_exceptions])): raise e\n",
                "                exceptions.append(e)\n",
                "                await asyncio.sleep(retry_delay)\n",
                "                    \n",
                "        if not success:\n",
                "            raise MaximumRetriesException(exceptions)\n",
                "        \n",
                "        # Call logging\n",
                "        if retrieve_log_data is not None:\n",
                "            if not retrieved_from_cache:\n",
                "                log_data = retrieve_log_data(model, func_args_and_kwargs, result)\n",
                "                _log_call( model=model, **log_data)\n",
                "        \n",
                "        return result\n",
                "    \n",
                "    llm_func.__name__ = func_name\n",
                "    return llm_func"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9d966158",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Maximum retries (5) reached. Exceptions:\n",
                        "0: (RateLimitError) litellm.RateLimitError: None\n",
                        "1: (RateLimitError) litellm.RateLimitError: None\n",
                        "2: (RateLimitError) litellm.RateLimitError: None\n",
                        "3: (RateLimitError) litellm.RateLimitError: None\n",
                        "4: (RateLimitError) litellm.RateLimitError: None\n"
                    ]
                }
            ],
            "source": [
                "#|hide\n",
                "async def foo(model):\n",
                "    raise litellm.RateLimitError(None, None, None)\n",
                "\n",
                "_foo = _llm_async_func_factory(\n",
                "    func=foo,\n",
                "    func_name=\"foo\",\n",
                "    func_cache_name=\"foo\",\n",
                ")\n",
                "\n",
                "try:\n",
                "    await _foo(model=\"foo\", retry_delay=0.01)\n",
                "except MaximumRetriesException as e:\n",
                "    print(e)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0843fd16",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Maximum retries (5) reached. Exceptions:\n",
                        "0: (TimeoutError) \n",
                        "1: (TimeoutError) \n",
                        "2: (TimeoutError) \n",
                        "3: (TimeoutError) \n",
                        "4: (TimeoutError) \n"
                    ]
                }
            ],
            "source": [
                "#|hide\n",
                "async def bar(model):\n",
                "    await asyncio.sleep(10)\n",
                "\n",
                "_foo = _llm_async_func_factory(\n",
                "    func=bar,\n",
                "    func_name=\"bar\",\n",
                "    func_cache_name=\"bar\",\n",
                ")\n",
                "\n",
                "try:\n",
                "    await _foo(model=\"bar\", retry_delay=0.01, timeout=0.01)\n",
                "except MaximumRetriesException as e:\n",
                "    print(e)"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        },
        "nblite_source_hash": "27981e2c9a830dace9aa71fafaf9207dc74880c2c657aef46e708050bffac0c4"
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
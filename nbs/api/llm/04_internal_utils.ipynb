{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "7adbfe75",
            "metadata": {},
            "source": [
                "# _utils"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ec4869ae",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|default_exp llm._utils"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "546ad7cd",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "import nblite; from nblite import show_doc; nblite.nbl_export()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f70577eb",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "try:\n",
                "    import litellm\n",
                "    import inspect\n",
                "    import functools\n",
                "    from typing import Callable, Optional, Union\n",
                "    from pathlib import Path\n",
                "    from adulib.llm.caching import _cache_execute, _async_cache_execute, get_cache_key, is_in_cache\n",
                "    from adulib.llm.call_logging import _log_call\n",
                "    from adulib.llm.rate_limits import _get_limiter\n",
                "except ImportError as e:\n",
                "    raise ImportError(f\"Install adulib[llm] to use this API.\") from e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dcacc75b",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "import adulib.llm._utils as this_module"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f1fb156b",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_cache_key(\n",
                "    model: str, method_name, content: any, cache_key_prepend: Union[str, None]=None, include_model_in_cache_key: bool=True\n",
                ") -> tuple:\n",
                "    return ('adulib.llm', method_name, cache_key_prepend, model if include_model_in_cache_key else '', content)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d0b7d3cb",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|exporti\n",
                "def _llm_func_factory(\n",
                "    func: Callable,\n",
                "    func_name: str,\n",
                "    func_cache_name: str,\n",
                "    retrieve_log_data: Optional[Callable] = None,\n",
                "):\n",
                "    func_sig = inspect.signature(func)\n",
                "    def llm_func(\n",
                "        *args,\n",
                "        cache_enabled: bool=True,\n",
                "        cache_path: Optional[Union[str, Path]]=None,\n",
                "        cache_key_prefix: Optional[str]=None,\n",
                "        include_model_in_cache_key: bool=True,\n",
                "        return_cache_key: bool=False,\n",
                "        **kwargs,\n",
                "    ):\n",
                "        # Generate cache key\n",
                "        bound = func_sig.bind(*args, **kwargs)\n",
                "        func_args_and_kwargs = dict(bound.arguments)\n",
                "        model = func_args_and_kwargs.pop('model') # we treat 'model' separately, as we can optionally exclude it from the cache key\n",
                "        cache_key = get_cache_key(model, func_cache_name, func_args_and_kwargs, cache_key_prefix, include_model_in_cache_key)\n",
                "        if return_cache_key: return cache_key\n",
                "        \n",
                "        # Caching\n",
                "        retrieved_from_cache, result = _cache_execute(\n",
                "            cache_key=cache_key,\n",
                "            execute_func=lambda: func(*args, **kwargs),\n",
                "            cache_enabled=cache_enabled,\n",
                "            cache_path=cache_path,\n",
                "        )\n",
                "        \n",
                "        # Call logging\n",
                "        if retrieve_log_data is not None:\n",
                "            if not retrieved_from_cache:\n",
                "                log_data = retrieve_log_data(model, func_args_and_kwargs, result)\n",
                "                _log_call( model=model, **log_data)\n",
                "        \n",
                "        return result\n",
                "    \n",
                "    llm_func.__name__ = func_name\n",
                "    return llm_func"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "32237801",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|exporti\n",
                "def _llm_async_func_factory(\n",
                "    func: Callable,\n",
                "    func_name: str,\n",
                "    func_cache_name: str,\n",
                "    retrieve_log_data: Optional[Callable] = None,\n",
                "):\n",
                "    func_sig = inspect.signature(func)\n",
                "    async def llm_func(\n",
                "        *args,\n",
                "        cache_enabled: bool=True,\n",
                "        cache_path: Optional[Union[str, Path]]=None,\n",
                "        cache_key_prefix: Optional[str]=None,\n",
                "        include_model_in_cache_key: bool=True,\n",
                "        return_cache_key: bool=False,\n",
                "        **kwargs,\n",
                "    ):\n",
                "        # Generate cache key\n",
                "        bound = func_sig.bind(*args, **kwargs)\n",
                "        func_args_and_kwargs = dict(bound.arguments)\n",
                "        model = func_args_and_kwargs.pop('model') # we treat 'model' separately, as we can optionally exclude it from the cache key\n",
                "        cache_key = get_cache_key(model, func_cache_name, func_args_and_kwargs, cache_key_prefix, include_model_in_cache_key)\n",
                "        if return_cache_key: return cache_key\n",
                "        \n",
                "        # Rate limiting\n",
                "        key_in_cache = is_in_cache(cache_key)\n",
                "        if not key_in_cache:\n",
                "            api_key = kwargs.get(\"api_key\", None)\n",
                "            await _get_limiter(model, api_key).wait()\n",
                "        \n",
                "        # Caching\n",
                "        retrieved_from_cache, result = await _async_cache_execute(\n",
                "            cache_key=cache_key,\n",
                "            execute_func=lambda: func(*args, **kwargs),\n",
                "            cache_enabled=cache_enabled,\n",
                "            cache_path=cache_path,\n",
                "        )\n",
                "        \n",
                "        # Call logging\n",
                "        if retrieve_log_data is not None:\n",
                "            if not retrieved_from_cache:\n",
                "                log_data = retrieve_log_data(model, func_args_and_kwargs, result)\n",
                "                _log_call(model=model, **log_data)\n",
                "        \n",
                "        return result\n",
                "    \n",
                "    llm_func.__name__ = func_name\n",
                "    return llm_func"
            ]
        }
    ],
    "metadata": {},
    "nbformat": 4,
    "nbformat_minor": 5
}
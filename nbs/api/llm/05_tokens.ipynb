{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "ff2ab17d",
            "metadata": {},
            "source": [
                "# tokens"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5dc1ceb0",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|default_exp llm.tokens"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7513b8fb",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "import nblite; from nblite import show_doc; nblite.nbl_export()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8b9e717b",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "try:\n",
                "    import litellm\n",
                "    from adulib.llm._utils import _llm_func_factory\n",
                "except ImportError as e:\n",
                "    raise ImportError(f\"Install adulib[llm] to use this API.\") from e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6e972275",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "from adulib.caching import set_default_cache_path\n",
                "from adulib.llm.caching import is_in_cache, clear_cache_key\n",
                "import adulib.llm.tokens as this_module"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "24d7d7f0",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "repo_path = nblite.config.get_project_root_and_config()[0]\n",
                "set_default_cache_path(repo_path / '.tmp_cache')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4163df12",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "## token_counter\n",
                            "\n",
                            "```python\n",
                            "token_counter(model: None, custom_tokenizer: typing.Union[dict, litellm.types.utils.SelectTokenizerResponse, NoneType], text: typing.Union[str, typing.List[str], NoneType], messages: typing.Optional[typing.List], count_response_tokens: typing.Optional[bool], tools: typing.Optional[typing.List[litellm.types.llms.openai.ChatCompletionToolParam]], tool_choice: typing.Optional[litellm.types.llms.openai.ChatCompletionNamedToolChoiceParam], use_default_image_token_count: typing.Optional[bool], default_token_count: typing.Optional[int])\n",
                            "```\n",
                            "\n",
                            "The same as `litellm.litellm_core_utils.token_counter`.\n",
                            "\n",
                            "Kept for backwards compatibility.\n",
                            "\n",
                            "---\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#|echo: false\n",
                "show_doc(this_module.token_counter)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7b453927",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "token_counter = _llm_func_factory(\n",
                "    func=litellm.token_counter,\n",
                "    func_name=\"token_counter\",\n",
                "    func_cache_name=\"token_counter\",\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8f101f54",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "0\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "13"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "token_counter(\n",
                "    model=\"gpt-3.5-turbo\",\n",
                "    messages=[\n",
                "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
                "    ]\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "00d46bee",
            "metadata": {},
            "source": [
                "If we set `return_cache_key=True`, the function is not executed and only the cache key is returned instead."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1a7796a9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# This will not execute the function, but only return the cache key.\n",
                "cache_key = token_counter(\n",
                "    model=\"gpt-4o\",\n",
                "    text=\"Hello, how are you?\",\n",
                "    return_cache_key=True\n",
                ")\n",
                "\n",
                "assert not is_in_cache(cache_key)\n",
                "\n",
                "# This will cache the result.\n",
                "token_counter(\n",
                "    model=\"gpt-4o\",\n",
                "    text=\"Hello, how are you?\"\n",
                ")\n",
                "\n",
                "assert is_in_cache(cache_key)\n",
                "clear_cache_key(cache_key)"
            ]
        }
    ],
    "metadata": {},
    "nbformat": 4,
    "nbformat_minor": 5
}
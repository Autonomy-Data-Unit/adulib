{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "ff2ab17d",
            "metadata": {},
            "source": [
                "# tokens"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5dc1ceb0",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|default_exp llm.tokens"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7513b8fb",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "import nblite; from nblite import show_doc; nblite.nbl_export()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8b9e717b",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "try:\n",
                "    import litellm\n",
                "    from adulib.llm._utils import _llm_func_factory\n",
                "except ImportError as e:\n",
                "    raise ImportError(f\"Install adulib[llm] to use this API.\") from e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6e972275",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "from adulib.caching import set_default_cache_path\n",
                "from adulib.llm.caching import is_in_cache, clear_cache_key\n",
                "import adulib.llm.tokens as this_module"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "24d7d7f0",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "repo_path = nblite.config.get_project_root_and_config()[0]\n",
                "set_default_cache_path(repo_path / '.tmp_cache')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4163df12",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "## token_counter\n",
                            "\n",
                            "```python\n",
                            "token_counter(\n",
                            "   *args,\n",
                            "   cache_enabled: bool,\n",
                            "   cache_path: typing.Union[str, pathlib.Path, NoneType],\n",
                            "   cache_key_prefix: typing.Optional[str],\n",
                            "   include_model_in_cache_key: bool,\n",
                            "   return_cache_key: bool,\n",
                            "   return_info: bool,\n",
                            "   enable_retries: bool,\n",
                            "   retry_on_exceptions: typing.Optional[list[Exception]],\n",
                            "   retry_on_all_exceptions: bool,\n",
                            "   max_retries: typing.Optional[int],\n",
                            "   retry_delay: typing.Optional[int],\n",
                            "   **kwargs\n",
                            ")\n",
                            "```\n",
                            "\n",
                            "---\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#|echo: false\n",
                "show_doc(this_module.token_counter)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7b453927",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "token_counter = _llm_func_factory(\n",
                "    func=litellm.token_counter,\n",
                "    func_name=\"token_counter\",\n",
                "    func_cache_name=\"token_counter\",\n",
                "    module_name=__name__,\n",
                "    cache_key_content_args=['messages', 'text'],\n",
                "    default_return_info=False,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8f101f54",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "13"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "token_counter(\n",
                "    model=\"gpt-3.5-turbo\",\n",
                "    messages=[\n",
                "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
                "    ]\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "00d46bee",
            "metadata": {},
            "source": [
                "If we set `return_cache_key=True`, the function is not executed and only the cache key is returned instead."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1a7796a9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# This will not execute the function, but only return the cache key.\n",
                "cache_key = token_counter(\n",
                "    model=\"gpt-4o\",\n",
                "    text=\"Hello, how are you?\",\n",
                "    return_cache_key=True,\n",
                ")\n",
                "\n",
                "clear_cache_key(cache_key, allow_non_existent=True)\n",
                "assert not is_in_cache(cache_key)\n",
                "\n",
                "# This will cache the result.\n",
                "num_tokens, cache_hit = token_counter(\n",
                "    model=\"gpt-4o\",\n",
                "    text=\"Hello, how are you?\",\n",
                "    return_info=True\n",
                ")\n",
                "assert not cache_hit\n",
                "\n",
                "num_tokens, cache_hit = token_counter(\n",
                "    model=\"gpt-4o\",\n",
                "    text=\"Hello, how are you?\",\n",
                "    return_info=True\n",
                ")\n",
                "assert cache_hit\n",
                "\n",
                "assert is_in_cache(cache_key)\n",
                "clear_cache_key(cache_key)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "adulib",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        },
        "nblite_source_hash": "30a35d99bd3d92cd7371435aeca0210cf812362d516dd46353fc17b5b54a1628"
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
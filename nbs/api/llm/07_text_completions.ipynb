{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "29b6ceb6",
            "metadata": {},
            "source": [
                "# text_completions\n",
                "\n",
                "> See the [`litellm` documention](https://docs.litellm.ai/docs/text_completion)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "be6ba6f5",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|default_exp llm.text_completions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d1abe544",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "import nblite; from nblite import show_doc; nblite.nbl_export()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "900efcec",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "try:\n",
                "    import litellm\n",
                "    import functools\n",
                "    from adulib.llm._utils import _llm_func_factory, _llm_async_func_factory\n",
                "    from adulib.llm.tokens import token_counter\n",
                "except ImportError as e:\n",
                "    raise ImportError(f\"Install adulib[llm] to use this API.\") from e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0e75a353",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "from adulib.caching import set_default_cache_path\n",
                "import adulib.llm.text_completions as this_module"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f24d467c",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "repo_path = nblite.config.get_project_root_and_config()[0]\n",
                "set_default_cache_path(repo_path / '.tmp_cache')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8af77f10",
            "metadata": {},
            "source": [
                "Text completions generate a continuation of a single prompt string, making them ideal for tasks like autocomplete, code completion, or single-turn text generation. This is contrast to chat completions, which are meant for multi-turn conversations, where the input is a list of messages with roles (like \"user\" and \"assistant\"), allowing the model to maintain context and produce more coherent, context-aware responses across multiple exchanges. Use text completions for simple, stateless tasks, and chat completions for interactive, context-dependent scenarios."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e760941d",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "## text_completion\n",
                            "\n",
                            "```python\n",
                            "text_completion(\n",
                            "   *args,\n",
                            "   cache_enabled: bool,\n",
                            "   cache_path: typing.Union[str, pathlib.Path, NoneType],\n",
                            "   cache_key_prefix: typing.Optional[str],\n",
                            "   include_model_in_cache_key: bool,\n",
                            "   return_cache_key: bool,\n",
                            "   enable_retries: bool,\n",
                            "   retry_on_exceptions: typing.Optional[list[Exception]],\n",
                            "   retry_on_all_exceptions: bool,\n",
                            "   max_retries: typing.Optional[int],\n",
                            "   retry_delay: typing.Optional[int],\n",
                            "   **kwargs\n",
                            ")\n",
                            "```\n",
                            "\n",
                            "This function is a wrapper around a corresponding function in the `litellm` library, see [this](https://docs.litellm.ai/docs/text_completion) for a full list of the available arguments.\n",
                            "\n",
                            "---\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#|echo: false\n",
                "show_doc(this_module.text_completion)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1022e5de",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "text_completion = _llm_func_factory(\n",
                "    func=litellm.text_completion,\n",
                "    func_name=\"text_completion\",\n",
                "    func_cache_name=\"text_completion\",\n",
                "    retrieve_log_data=lambda model, func_kwargs, response, cache_args: {\n",
                "        \"method\": \"text_completion\",\n",
                "        \"input_tokens\": token_counter(model=model, text=func_kwargs['prompt'], **cache_args),\n",
                "        \"output_tokens\": sum([token_counter(model=model, text=c.text, **cache_args) for c in response.choices]),\n",
                "        \"cost\": response._hidden_params['response_cost'],\n",
                "    }\n",
                ")\n",
                "\n",
                "text_completion.__doc__ = \"\"\"\n",
                "This function is a wrapper around a corresponding function in the `litellm` library, see [this](https://docs.litellm.ai/docs/text_completion) for a full list of the available arguments.\n",
                "\"\"\".strip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b05f3d6d",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'1 + 1 = 2.'"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "response = text_completion(\n",
                "    model=\"gpt-4o-mini\",\n",
                "    prompt=\"1 + 1 = \",\n",
                ")\n",
                "response.choices[0].text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "26a30901",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "## async_text_completion *(async)*\n",
                            "\n",
                            "```python\n",
                            "async_text_completion(\n",
                            "   *args,\n",
                            "   cache_enabled: bool,\n",
                            "   cache_path: typing.Union[str, pathlib.Path, NoneType],\n",
                            "   cache_key_prefix: typing.Optional[str],\n",
                            "   include_model_in_cache_key: bool,\n",
                            "   return_cache_key: bool,\n",
                            "   enable_retries: bool,\n",
                            "   retry_on_exceptions: typing.Optional[list[Exception]],\n",
                            "   retry_on_all_exceptions: bool,\n",
                            "   max_retries: typing.Optional[int],\n",
                            "   retry_delay: typing.Optional[int],\n",
                            "   timeout: typing.Optional[int],\n",
                            "   **kwargs\n",
                            ")\n",
                            "```\n",
                            "\n",
                            "---\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#|echo: false\n",
                "show_doc(this_module.async_text_completion)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a1307fab",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "async_text_completion = _llm_async_func_factory(\n",
                "    func=functools.wraps(litellm.text_completion)(litellm.atext_completion), # This is needed as 'litellm.atext_completion' lacks the right signature\n",
                "    func_name=\"async_text_completion\",\n",
                "    func_cache_name=\"text_completion\",\n",
                "    retrieve_log_data=lambda model, func_kwargs, response, cache_args: {\n",
                "        \"method\": \"text_completion\",\n",
                "        \"input_tokens\": token_counter(model=model, text=func_kwargs['prompt'], **cache_args),\n",
                "        \"output_tokens\": sum([token_counter(model=model, text=c.text, **cache_args) for c in response.choices]),\n",
                "        \"cost\": response._hidden_params['response_cost'],\n",
                "    }\n",
                ")\n",
                "\n",
                "text_completion.__doc__ = \"\"\"\n",
                "This function is a wrapper around a corresponding function in the `litellm` library, see [this](https://docs.litellm.ai/docs/text_completion) for a full list of the available arguments.\n",
                "\"\"\".strip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7627297e",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'1 + 2 = 3.'"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "response = await async_text_completion(\n",
                "    model=\"gpt-4o-mini\",\n",
                "    prompt=\"1 + 2 = \",\n",
                ")\n",
                "response.choices[0].text"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        },
        "nblite_source_hash": "79f543f16b1b52ca89fc7e99f6b8b24d2726ecc7ac64fb7c3a731618e98d3ddf"
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "87063841",
            "metadata": {},
            "source": [
                "# embeddings\n",
                "\n",
                "> See the [`litellm` documention](https://docs.litellm.ai/docs/embedding/supported_embedding)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1df00f51",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|default_exp llm.embeddings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "62a1b5d4",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "import nblite; from nblite import show_doc; nblite.nbl_export()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "800bde30",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "try:\n",
                "    import litellm\n",
                "    import functools\n",
                "    from adulib.llm._utils import _llm_func_factory, _llm_async_func_factory\n",
                "    from adulib.llm.tokens import token_counter\n",
                "except ImportError as e:\n",
                "    raise ImportError(f\"Install adulib[llm] to use this API.\") from e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f9a0b288",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "from adulib.caching import set_default_cache_path\n",
                "import adulib.llm.embeddings as this_module"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3c4658d2",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "repo_path = nblite.config.get_project_root_and_config()[0]\n",
                "set_default_cache_path(repo_path / '.tmp_cache')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "67e0cff0",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "## embedding\n",
                            "\n",
                            "```python\n",
                            "embedding(\n",
                            "   *args,\n",
                            "   cache_enabled: bool,\n",
                            "   cache_path: typing.Union[str, pathlib.Path, NoneType],\n",
                            "   cache_key_prefix: typing.Optional[str],\n",
                            "   include_model_in_cache_key: bool,\n",
                            "   return_cache_key: bool,\n",
                            "   enable_retries: bool,\n",
                            "   retry_on_exceptions: typing.Optional[list[Exception]],\n",
                            "   retry_on_all_exceptions: bool,\n",
                            "   max_retries: typing.Optional[int],\n",
                            "   retry_delay: typing.Optional[int],\n",
                            "   **kwargs\n",
                            ")\n",
                            "```\n",
                            "\n",
                            "This function is a wrapper around a corresponding function in the `litellm` library, see [this](https://docs.litellm.ai/docs/embedding/supported_embedding) for a full list of the available arguments.\n",
                            "\n",
                            "---\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#|echo: false\n",
                "show_doc(this_module.embedding)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "078c67a9",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "embedding = _llm_func_factory(\n",
                "    func=litellm.embedding,\n",
                "    func_name=\"embedding\",\n",
                "    func_cache_name=\"embedding\",\n",
                "    retrieve_log_data=lambda model, func_kwargs, response: {\n",
                "        \"method\": \"embedding\",\n",
                "        \"input_tokens\": sum([token_counter(model=model, text=inp) for inp in func_kwargs['input']]),\n",
                "        \"output_tokens\": None,\n",
                "        \"cost\": response._hidden_params['response_cost'],\n",
                "    }\n",
                ")\n",
                "\n",
                "embedding.__doc__ = \"\"\"\n",
                "This function is a wrapper around a corresponding function in the `litellm` library, see [this](https://docs.litellm.ai/docs/embedding/supported_embedding) for a full list of the available arguments.\n",
                "\"\"\".strip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cface48a",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[-0.0012842135038226843,\n",
                            " -0.013222426176071167,\n",
                            " -0.008362501859664917,\n",
                            " -0.04306064546108246,\n",
                            " -0.004547890741378069,\n",
                            " 0.003748304443433881,\n",
                            " 0.03082892671227455,\n",
                            " -0.012777778320014477,\n",
                            " -0.01638176664710045,\n",
                            " -0.01972052827477455]"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "response = embedding(\n",
                "    model=\"text-embedding-3-small\",\n",
                "    input=[\n",
                "        \"First string to embsed\",\n",
                "        \"Second string to embed\",\n",
                "    ],\n",
                ")\n",
                "response.data[1]['embedding'][:10]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c0351122",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "## async_embedding *(async)*\n",
                            "\n",
                            "```python\n",
                            "async_embedding(\n",
                            "   *args,\n",
                            "   cache_enabled: bool,\n",
                            "   cache_path: typing.Union[str, pathlib.Path, NoneType],\n",
                            "   cache_key_prefix: typing.Optional[str],\n",
                            "   include_model_in_cache_key: bool,\n",
                            "   return_cache_key: bool,\n",
                            "   enable_retries: bool,\n",
                            "   retry_on_exceptions: typing.Optional[list[Exception]],\n",
                            "   retry_on_all_exceptions: bool,\n",
                            "   max_retries: typing.Optional[int],\n",
                            "   retry_delay: typing.Optional[int],\n",
                            "   timeout: typing.Optional[int],\n",
                            "   **kwargs\n",
                            ")\n",
                            "```\n",
                            "\n",
                            "---\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#|echo: false\n",
                "show_doc(this_module.async_embedding)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9bbfadc8",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "async_embedding = _llm_async_func_factory(\n",
                "    func=functools.wraps(litellm.embedding)(litellm.aembedding), # This is needed as 'litellm.aembedding' lacks the right signature\n",
                "    func_name=\"async_embedding\",\n",
                "    func_cache_name=\"embedding\",\n",
                "    retrieve_log_data=lambda model, func_kwargs, response: {\n",
                "        \"method\": \"embedding\",\n",
                "        \"input_tokens\": sum([token_counter(model=model, text=inp) for inp in func_kwargs['input']]),\n",
                "        \"output_tokens\": None,\n",
                "        \"cost\": response._hidden_params['response_cost'],\n",
                "    }\n",
                ")\n",
                "\n",
                "embedding.__doc__ = \"\"\"\n",
                "This function is a wrapper around a corresponding function in the `litellm` library, see [this](https://docs.litellm.ai/docs/embedding/supported_embedding) for a full list of the available arguments.\n",
                "\"\"\".strip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7656b28b",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[-0.0012842135038226843,\n",
                            " -0.013222426176071167,\n",
                            " -0.008362501859664917,\n",
                            " -0.04306064546108246,\n",
                            " -0.004547890741378069,\n",
                            " 0.003748304443433881,\n",
                            " 0.03082892671227455,\n",
                            " -0.012777778320014477,\n",
                            " -0.01638176664710045,\n",
                            " -0.01972052827477455]"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "response = await async_embedding(\n",
                "    model=\"text-embedding-3-small\",\n",
                "    input=[\n",
                "        \"First string to embsed\",\n",
                "        \"Second string to embed\",\n",
                "    ],\n",
                ")\n",
                "response.data[1]['embedding'][:10]"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        },
        "nblite_source_hash": "2929913d51519ad4c333aa4958fecb6a55279e0d3375620d133af69979783abb"
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "801832fd",
            "metadata": {},
            "source": [
                "# completions\n",
                "\n",
                "> Otherwise known as *chat completions*. See the [`litellm` documention](https://docs.litellm.ai/docs/completion)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f0ff259f",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|default_exp llm.completions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4add820c",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "import nblite; from nblite import show_doc; nblite.nbl_export()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9131f197",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "try:\n",
                "    import litellm\n",
                "    import inspect\n",
                "    import functools\n",
                "    from typing import List\n",
                "    from adulib.llm._utils import _llm_func_factory, _llm_async_func_factory\n",
                "    from adulib.llm.tokens import token_counter\n",
                "except ImportError as e:\n",
                "    raise ImportError(f\"Install adulib[llm] to use this API.\") from e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9cba312c",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "from pydantic import BaseModel\n",
                "from adulib.caching import set_default_cache_path\n",
                "import adulib.llm.completions as this_module"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d475458f",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "repo_path = nblite.config.get_project_root_and_config()[0]\n",
                "set_default_cache_path(repo_path / '.tmp_cache')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b0009398",
            "metadata": {},
            "source": [
                "The two required fields for completions are `model` and `message`. Some optional arguments are:\n",
                "\n",
                "#### Properties of `messages`\n",
                "\n",
                "Each message in the `messages` array can include the following fields:\n",
                "\n",
                "- `role: str` (**required**) - The role of the message's author. Roles can be: system, user, assistant, function or tool.\n",
                "- `content: Union[str,List[dict],None]` (**required**) - The contents of the message. It is required for all messages, but may be null for assistant messages with function calls.\n",
                "- `name: str` - The name of the author of the message. It is required if the role is \"function\". The name should match the name of the function represented in the content. It can contain characters (a-z, A-Z, 0-9), and underscores, with a maximum length of 64 characters.\n",
                "- `function_call: object` - The name and arguments of a function that should be called, as generated by the model.\n",
                "- `tool_call_id: str` - Tool call that this message is responding to.\n",
                "\n",
                "\n",
                "#### Explanation of roles\n",
                "\n",
                "- **system**: Sets assistant context. Example: `{ \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }`\n",
                "- **user**: End user input. Example:  `{ \"role\": \"user\", \"content\": \"What's the weather like today?\" }`\n",
                "- **assistant**: AI response. Example: `{ \"role\": \"assistant\", \"content\": \"The weather is sunny and warm.\" }`\n",
                "- **function**: Function call/result (`name` required). Example:  `{ \"role\": \"function\", \"name\": \"get_weather\", \"content\": \"{\\\"location\\\": \\\"San Francisco\\\"}\" }`\n",
                "- **tool**: Tool/plugin interaction (`tool_call_id` required). Example: `{ \"role\": \"tool\", \"tool_call_id\": \"abc123\", \"content\": \"Tool response here\" }`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "95e464fd",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "## completion\n",
                            "\n",
                            "```python\n",
                            "completion(\n",
                            "   *args,\n",
                            "   cache_enabled: bool,\n",
                            "   cache_path: typing.Union[str, pathlib.Path, NoneType],\n",
                            "   cache_key_prefix: typing.Optional[str],\n",
                            "   include_model_in_cache_key: bool,\n",
                            "   return_cache_key: bool,\n",
                            "   **kwargs\n",
                            ")\n",
                            "```\n",
                            "\n",
                            "This function is a wrapper around a corresponding function in the `litellm` library, see [this](https://docs.litellm.ai/docs/completion/input) for a full list of the available arguments.\n",
                            "\n",
                            "---\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#|echo: false\n",
                "show_doc(this_module.completion)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8a0efc70",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "completion = _llm_func_factory(\n",
                "    func=litellm.completion,\n",
                "    func_name=\"completion\",\n",
                "    func_cache_name=\"completion\",\n",
                "    retrieve_log_data=lambda model, func_kwargs, response: {\n",
                "        \"method\": \"completion\",\n",
                "        \"input_tokens\": token_counter(model=model, messages=func_kwargs['messages']),\n",
                "        \"output_tokens\": sum([token_counter(model=model, messages=[{'role': c.message.role, 'content': c.message.content}]) for c in response.choices]),\n",
                "        \"cost\": response._hidden_params['response_cost'],\n",
                "    }\n",
                ")\n",
                "\n",
                "completion.__doc__ = \"\"\"\n",
                "This function is a wrapper around a corresponding function in the `litellm` library, see [this](https://docs.litellm.ai/docs/completion/input) for a full list of the available arguments.\n",
                "\"\"\".strip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2d756d81",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'The capital of France is Paris.'"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "response = completion(\n",
                "    model=\"gpt-4o-mini\",\n",
                "    messages=[\n",
                "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
                "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
                "    ],\n",
                ")\n",
                "response.choices[0].message.content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ca783c27",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'name': 'Simple Pancakes',\n",
                            " 'ingredients': ['1 cup all-purpose flour',\n",
                            "  '2 tablespoons sugar',\n",
                            "  '2 teaspoons baking powder',\n",
                            "  '1/2 teaspoon salt',\n",
                            "  '1 cup milk',\n",
                            "  '1 egg',\n",
                            "  '2 tablespoons melted butter',\n",
                            "  '1 teaspoon vanilla extract'],\n",
                            " 'steps': ['In a large mixing bowl, whisk together the flour, sugar, baking powder, and salt.',\n",
                            "  'In another bowl, combine the milk, egg, melted butter, and vanilla extract.',\n",
                            "  'Pour the wet ingredients into the dry ingredients and stir until just combined. (A few lumps are okay.)',\n",
                            "  'Preheat a non-stick skillet or griddle over medium heat. Lightly grease with butter or oil if needed.',\n",
                            "  'Pour about 1/4 cup of batter onto the skillet for each pancake.',\n",
                            "  'Cook until bubbles form on the surface and the edges look set, about 2-3 minutes.',\n",
                            "  'Flip and cook for another 1-2 minutes until golden brown.',\n",
                            "  'Serve warm with your favorite toppings such as syrup, fruit, or whipped cream.']}"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "class Recipe(BaseModel):\n",
                "    name: str\n",
                "    ingredients: List[str]\n",
                "    steps: List[str]\n",
                "\n",
                "response = completion(\n",
                "    model=\"gpt-4o-mini\",\n",
                "    messages=[\n",
                "        {\"role\": \"system\", \"content\": \"You are a helpful cooking assistant.\"},\n",
                "        {\"role\": \"user\", \"content\": \"Give me a simple recipe for pancakes.\"}\n",
                "    ],\n",
                "    response_format=Recipe\n",
                ")\n",
                "\n",
                "Recipe.model_validate_json(response.choices[0].message.content).model_dump()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "edf02c95",
            "metadata": {},
            "source": [
                "You can save costs during testing using [mock responses](https://docs.litellm.ai/docs/completion/mock_requests):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "15b85c3d",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'Stockholm'"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "response = completion(\n",
                "    model=\"gpt-4o-mini\",\n",
                "    messages=[\n",
                "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
                "        {\"role\": \"user\", \"content\": \"What is the capital of Sweden?\"}\n",
                "    ],\n",
                "    mock_response = \"Stockholm\"\n",
                ")\n",
                "response.choices[0].message.content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5ec5f422",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "## async_completion *(async)*\n",
                            "\n",
                            "```python\n",
                            "async_completion(\n",
                            "   model: str,\n",
                            "   *args,\n",
                            "   cache_enabled: bool,\n",
                            "   cache_path: typing.Union[str, pathlib.Path, NoneType],\n",
                            "   cache_key_prefix: typing.Optional[str],\n",
                            "   include_model_in_cache_key: bool,\n",
                            "   return_cache_key: bool,\n",
                            "   **kwargs\n",
                            ")\n",
                            "```\n",
                            "\n",
                            "---\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#|echo: false\n",
                "show_doc(this_module.async_completion)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e3fca6a9",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "async_completion = _llm_async_func_factory(\n",
                "    func=litellm.acompletion,\n",
                "    func_name=\"async_completion\",\n",
                "    func_cache_name=\"completion\",\n",
                "    retrieve_log_data=lambda model, func_kwargs, response: {\n",
                "        \"method\": \"completion\",\n",
                "        \"input_tokens\": token_counter(model=model, messages=func_kwargs['messages']),\n",
                "        \"output_tokens\": sum([token_counter(model=model, messages=[{'role': c.message.role, 'content': c.message.content}]) for c in response.choices]),\n",
                "        \"cost\": response._hidden_params['response_cost'],\n",
                "    }\n",
                ")\n",
                "\n",
                "completion.__doc__ = \"\"\"\n",
                "This function is a wrapper around a corresponding function in the `litellm` library, see [this](https://docs.litellm.ai/docs/completion/input) for a full list of the available arguments.\n",
                "\"\"\".strip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "162a84af",
            "metadata": {},
            "outputs": [
                {
                    "ename": "TypeError",
                    "evalue": "missing a required argument: 'model'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m async_completion(\n\u001b[32m      2\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgpt-3.5-turbo\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     messages=[\n\u001b[32m      4\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mYou are a helpful assistant.\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m      5\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWhat is the capital of France?\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m      6\u001b[39m     ],\n\u001b[32m      7\u001b[39m )\n\u001b[32m      8\u001b[39m response.choices[\u001b[32m0\u001b[39m].message.content\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/dev-adu/adulib/adulib/llm/_utils.py:81\u001b[39m, in \u001b[36m_llm_async_func_factory.<locals>.llm_func\u001b[39m\u001b[34m(model, cache_enabled, cache_path, cache_key_prefix, include_model_in_cache_key, return_cache_key, *args, **kwargs)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mllm_func\u001b[39m(\n\u001b[32m     71\u001b[39m     model: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     72\u001b[39m     *args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     79\u001b[39m ):\n\u001b[32m     80\u001b[39m     \u001b[38;5;66;03m# Generate cache key\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     bound = \u001b[43mfunc_sig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     func_args_and_kwargs = \u001b[38;5;28mdict\u001b[39m(bound.arguments)\n\u001b[32m     83\u001b[39m     model = func_args_and_kwargs.pop(\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;66;03m# we treat 'model' separately, as we can optionally exclude it from the cache key\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/inspect.py:3195\u001b[39m, in \u001b[36mSignature.bind\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args, **kwargs):\n\u001b[32m   3191\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[32m   3192\u001b[39m \u001b[33;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[32m   3193\u001b[39m \u001b[33;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[32m   3194\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/inspect.py:3110\u001b[39m, in \u001b[36mSignature._bind\u001b[39m\u001b[34m(self, args, kwargs, partial)\u001b[39m\n\u001b[32m   3108\u001b[39m                 msg = \u001b[33m'\u001b[39m\u001b[33mmissing a required argument: \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m   3109\u001b[39m                 msg = msg.format(arg=param.name)\n\u001b[32m-> \u001b[39m\u001b[32m3110\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3111\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3112\u001b[39m     \u001b[38;5;66;03m# We have a positional argument to process\u001b[39;00m\n\u001b[32m   3113\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
                        "\u001b[31mTypeError\u001b[39m: missing a required argument: 'model'"
                    ]
                }
            ],
            "source": [
                "response = await async_completion(\n",
                "    model=\"gpt-3.5-turbo\",\n",
                "    messages=[\n",
                "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
                "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
                "    ],\n",
                ")\n",
                "response.choices[0].message.content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "86c7b91a",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "## async_prompt\n",
                            "\n",
                            "```python\n",
                            "async_prompt(\n",
                            "   model: str,\n",
                            "   *args,\n",
                            "   cache_enabled: bool,\n",
                            "   cache_path: typing.Union[str, pathlib.Path, NoneType],\n",
                            "   cache_key_prefix: typing.Optional[str],\n",
                            "   include_model_in_cache_key: bool,\n",
                            "   return_cache_key: bool,\n",
                            "   **kwargs\n",
                            ")\n",
                            "```\n",
                            "\n",
                            "Simplified chat completions designed for single-turn tasks like classification, summarization, or extraction. For a full list of the available arguments see the [documentation](https://docs.litellm.ai/docs/completion/input) for the `completion` function in `litellm`.\n",
                            "\n",
                            "---\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#|echo: false\n",
                "show_doc(this_module.prompt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "550e2ec6",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "@functools.wraps(completion)\n",
                "def prompt(\n",
                "    model: str,\n",
                "    context: str,\n",
                "    prompt: str,\n",
                "    *args,\n",
                "    return_full_response: bool=False,\n",
                "    **kwargs,\n",
                "):\n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": context},\n",
                "        {\"role\": \"user\", \"content\": prompt}\n",
                "    ]\n",
                "    response = completion(model, messages, *args, **kwargs)\n",
                "    if return_full_response: return response\n",
                "    else: return response.choices[0].message.content\n",
                "    \n",
                "sig = inspect.signature(completion)\n",
                "prompt.__signature__ = sig.replace(parameters=[p for p in sig.parameters.values() if p.name != 'messages'])\n",
                "prompt.__name__ = \"prompt\"\n",
                "prompt.__doc__ = \"\"\"\n",
                "Simplified chat completions designed for single-turn tasks like classification, summarization, or extraction. For a full list of the available arguments see the [documentation](https://docs.litellm.ai/docs/completion/input) for the `completion` function in `litellm`.\n",
                "\"\"\".strip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fcd5c110",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'The capital of France is Paris.'"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "prompt(\n",
                "    model='gpt-4o-mini',\n",
                "    context='You are a helpful assistant.',\n",
                "    prompt='What is the capital of France?',\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4530b135",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "## completion *(async)*\n",
                            "\n",
                            "```python\n",
                            "completion(\n",
                            "   model: str,\n",
                            "   *args,\n",
                            "   cache_enabled: bool,\n",
                            "   cache_path: typing.Union[str, pathlib.Path, NoneType],\n",
                            "   cache_key_prefix: typing.Optional[str],\n",
                            "   include_model_in_cache_key: bool,\n",
                            "   return_cache_key: bool,\n",
                            "   **kwargs\n",
                            ")\n",
                            "```\n",
                            "\n",
                            "This function is a wrapper around a corresponding function in the `litellm` library, see [this](https://docs.litellm.ai/docs/completion/input) for a full list of the available arguments.\n",
                            "\n",
                            "---\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#|echo: false\n",
                "show_doc(this_module.async_prompt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "42430b79",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "@functools.wraps(completion)\n",
                "async def async_prompt(\n",
                "    model: str,\n",
                "    context: str,\n",
                "    prompt: str,\n",
                "    *args,\n",
                "    return_full_response: bool=False,\n",
                "    **kwargs,\n",
                "):\n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": context},\n",
                "        {\"role\": \"user\", \"content\": prompt}\n",
                "    ]\n",
                "    response = await async_completion(model, messages, *args, **kwargs)\n",
                "    if return_full_response: return response\n",
                "    else: return response.choices[0].message.content\n",
                "    \n",
                "sig = inspect.signature(completion)\n",
                "prompt.__signature__ = sig.replace(parameters=[p for p in sig.parameters.values() if p.name != 'messages'])\n",
                "prompt.__name__ = \"async_prompt\"\n",
                "prompt.__doc__ = \"\"\"\n",
                "Simplified chat completions designed for single-turn tasks like classification, summarization, or extraction. For a full list of the available arguments see the [documentation](https://docs.litellm.ai/docs/completion/input) for the `completion` function in `litellm`.\n",
                "\"\"\".strip()"
            ]
        }
    ],
    "metadata": {},
    "nbformat": 4,
    "nbformat_minor": 5
}
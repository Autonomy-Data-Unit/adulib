{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "801832fd",
            "metadata": {},
            "source": [
                "# completions\n",
                "\n",
                "> Otherwise known as *chat completions*. See the [`litellm` documention](https://docs.litellm.ai/docs/completion)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f0ff259f",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|default_exp llm.completions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4add820c",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "import nblite; from nblite import show_doc; nblite.nbl_export()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9131f197",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "try:\n",
                "    import litellm\n",
                "    import inspect\n",
                "    from inspect import Parameter\n",
                "    import functools\n",
                "    from typing import List, Dict\n",
                "    from adulib.llm._utils import _llm_func_factory, _llm_async_func_factory\n",
                "    from adulib.llm.tokens import token_counter\n",
                "except ImportError as e:\n",
                "    raise ImportError(f\"Install adulib[llm] to use this API.\") from e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9cba312c",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "from pydantic import BaseModel\n",
                "from adulib.caching import set_default_cache_path\n",
                "from adulib.asynchronous import batch_executor\n",
                "from adulib.utils import as_dict\n",
                "import adulib.llm.completions as this_module"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d475458f",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|hide\n",
                "from adulib.llm import set_call_log_save_path\n",
                "repo_path = nblite.config.get_project_root_and_config()[0]\n",
                "set_default_cache_path(repo_path / '.tmp_cache')\n",
                "set_call_log_save_path(repo_path / '.call_logs.jsonl') "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b0009398",
            "metadata": {},
            "source": [
                "The two required fields for completions are `model` and `message`. Some optional arguments are:\n",
                "\n",
                "#### Properties of `messages`\n",
                "\n",
                "Each message in the `messages` array can include the following fields:\n",
                "\n",
                "- `role: str` (**required**) - The role of the message's author. Roles can be: system, user, assistant, function or tool.\n",
                "- `content: Union[str,List[dict],None]` (**required**) - The contents of the message. It is required for all messages, but may be null for assistant messages with function calls.\n",
                "- `name: str` - The name of the author of the message. It is required if the role is \"function\". The name should match the name of the function represented in the content. It can contain characters (a-z, A-Z, 0-9), and underscores, with a maximum length of 64 characters.\n",
                "- `function_call: object` - The name and arguments of a function that should be called, as generated by the model.\n",
                "- `tool_call_id: str` - Tool call that this message is responding to.\n",
                "\n",
                "\n",
                "#### Explanation of roles\n",
                "\n",
                "- **system**: Sets assistant context. Example: `{ \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }`\n",
                "- **user**: End user input. Example:  `{ \"role\": \"user\", \"content\": \"What's the weather like today?\" }`\n",
                "- **assistant**: AI response. Example: `{ \"role\": \"assistant\", \"content\": \"The weather is sunny and warm.\" }`\n",
                "- **function**: Function call/result (`name` required). Example:  `{ \"role\": \"function\", \"name\": \"get_weather\", \"content\": \"{\\\"location\\\": \\\"San Francisco\\\"}\" }`\n",
                "- **tool**: Tool/plugin interaction (`tool_call_id` required). Example: `{ \"role\": \"tool\", \"tool_call_id\": \"abc123\", \"content\": \"Tool response here\" }`\n",
                "\n",
                "#### Simplified completions: `prompt`\n",
                "\n",
                "Use the `llm.prompt` (async: `llm.async_prompt`) to perform a simplified single-turn completion."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "95e464fd",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "## completion\n",
                            "\n",
                            "```python\n",
                            "completion(\n",
                            "   model: str,\n",
                            "   messages: typing.List[typing.Dict[str, str]],\n",
                            "   *args,\n",
                            "   cache_enabled: bool,\n",
                            "   cache_path: typing.Union[str, pathlib.Path, NoneType],\n",
                            "   cache_key_prefix: typing.Optional[str],\n",
                            "   include_model_in_cache_key: bool,\n",
                            "   return_cache_key: bool,\n",
                            "   return_info: bool,\n",
                            "   enable_retries: bool,\n",
                            "   retry_on_exceptions: typing.Optional[list[Exception]],\n",
                            "   retry_on_all_exceptions: bool,\n",
                            "   max_retries: typing.Optional[int],\n",
                            "   retry_delay: typing.Optional[int],\n",
                            "   **kwargs\n",
                            ")\n",
                            "```\n",
                            "\n",
                            "This function is a wrapper around a corresponding function in the `litellm` library, see [this](https://docs.litellm.ai/docs/completion/input) for a full list of the available arguments.\n",
                            "\n",
                            "---\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#|echo: false\n",
                "show_doc(this_module.completion)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8a0efc70",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "completion = _llm_func_factory(\n",
                "    func=litellm.completion,\n",
                "    func_name=\"completion\",\n",
                "    func_cache_name=\"completion\",\n",
                "    module_name=__name__,\n",
                "    cache_key_content_args=['messages', 'response_format'],\n",
                "    retrieve_log_data=lambda model, func_kwargs, response, cache_args: {\n",
                "        \"method\": \"completion\",\n",
                "        \"input_tokens\": token_counter(model=model, messages=func_kwargs['messages'], **cache_args),\n",
                "        \"output_tokens\": sum([token_counter(model=model, messages=[{'role': c.message.role, 'content': c.message.content}], **cache_args) for c in response.choices]),\n",
                "        \"cost\": response._hidden_params['response_cost'],\n",
                "    }\n",
                ")\n",
                "\n",
                "completion.__doc__ = \"\"\"\n",
                "This function is a wrapper around a corresponding function in the `litellm` library, see [this](https://docs.litellm.ai/docs/completion/input) for a full list of the available arguments.\n",
                "\"\"\".strip()\n",
                "\n",
                "sig = inspect.signature(completion)\n",
                "sig = sig.replace(parameters=[\n",
                "    Parameter(\"model\", Parameter.POSITIONAL_OR_KEYWORD, annotation=str),\n",
                "    Parameter(\"messages\", Parameter.POSITIONAL_OR_KEYWORD, annotation=List[Dict[str, str]]),\n",
                "    *sig.parameters.values()\n",
                "])\n",
                "completion.__signature__ = sig"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2d756d81",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'The capital of France is Paris.'"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "response, cache_hit, call_log = completion(\n",
                "    model=\"gpt-4o-mini\",\n",
                "    messages=[\n",
                "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
                "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
                "    ],\n",
                ")\n",
                "response.choices[0].message.content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0df0079e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Cache hit: True\n",
                        "Input tokens: 24\n",
                        "Output tokens: 14\n",
                        "Cost: 7.8e-06\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Cache hit: {cache_hit}\")\n",
                "print(f\"Input tokens: {call_log['input_tokens']}\")\n",
                "print(f\"Output tokens: {call_log['output_tokens']}\")\n",
                "print(f\"Cost: {call_log['cost']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "29d377d0",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'The capital of France is Paris.'"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "response = completion(\n",
                "    model=\"gpt-4o-mini\",\n",
                "    messages=[\n",
                "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
                "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
                "    ],\n",
                "    return_info=False\n",
                ")\n",
                "response.choices[0].message.content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ca783c27",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'name': 'Simple Pancakes',\n",
                            " 'ingredients': ['1 cup all-purpose flour',\n",
                            "  '2 tablespoons sugar',\n",
                            "  '2 teaspoons baking powder',\n",
                            "  '1/2 teaspoon salt',\n",
                            "  '1 cup milk',\n",
                            "  '1 egg',\n",
                            "  '2 tablespoons melted butter',\n",
                            "  '1 teaspoon vanilla extract'],\n",
                            " 'steps': ['In a large bowl, whisk together the flour, sugar, baking powder, and salt.',\n",
                            "  'In a separate bowl, mix the milk, egg, melted butter, and vanilla extract until well combined.',\n",
                            "  \"Pour the wet ingredients into the dry ingredients and stir until just combined. Do not overmix; it's okay if there are a few lumps.\",\n",
                            "  'Heat a non-stick skillet or griddle over medium heat and grease lightly with butter or oil.',\n",
                            "  'Pour 1/4 cup of batter onto the skillet for each pancake. Cook until bubbles form on the surface, about 2-3 minutes.',\n",
                            "  'Flip the pancakes and cook for another 2-3 minutes, until golden brown.',\n",
                            "  'Remove from skillet and keep warm while cooking the remaining pancakes.']}"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "class Recipe(BaseModel):\n",
                "    name: str\n",
                "    ingredients: List[str]\n",
                "    steps: List[str]\n",
                "\n",
                "response, cache_hit, call_log = completion(\n",
                "    model=\"gpt-4o-mini\",\n",
                "    messages=[\n",
                "        {\"role\": \"system\", \"content\": \"You are a helpful cooking assistant.\"},\n",
                "        {\"role\": \"user\", \"content\": \"Give me a simple recipe for pancakes.\"}\n",
                "    ],\n",
                "    response_format=Recipe\n",
                ")\n",
                "\n",
                "Recipe.model_validate_json(response.choices[0].message.content).model_dump()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "edf02c95",
            "metadata": {},
            "source": [
                "You can save costs during testing using [mock responses](https://docs.litellm.ai/docs/completion/mock_requests):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "15b85c3d",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'Stockholm'"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "response, cache_hit, call_log = completion(\n",
                "    model=\"gpt-4o-mini\",\n",
                "    messages=[\n",
                "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
                "        {\"role\": \"user\", \"content\": \"What is the capital of Sweden?\"}\n",
                "    ],\n",
                "    mock_response = \"Stockholm\"\n",
                ")\n",
                "response.choices[0].message.content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5ec5f422",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "## async_completion *(async)*\n",
                            "\n",
                            "```python\n",
                            "async_completion(\n",
                            "   model: str,\n",
                            "   messages: typing.List[typing.Dict[str, str]],\n",
                            "   *args,\n",
                            "   cache_enabled: bool,\n",
                            "   cache_path: typing.Union[str, pathlib.Path, NoneType],\n",
                            "   cache_key_prefix: typing.Optional[str],\n",
                            "   include_model_in_cache_key: bool,\n",
                            "   return_cache_key: bool,\n",
                            "   return_info: bool,\n",
                            "   enable_retries: bool,\n",
                            "   retry_on_exceptions: typing.Optional[list[Exception]],\n",
                            "   retry_on_all_exceptions: bool,\n",
                            "   max_retries: typing.Optional[int],\n",
                            "   retry_delay: typing.Optional[int],\n",
                            "   timeout: typing.Optional[int],\n",
                            "   **kwargs\n",
                            ")\n",
                            "```\n",
                            "\n",
                            "---\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#|echo: false\n",
                "show_doc(this_module.async_completion)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e3fca6a9",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "async_completion = _llm_async_func_factory(\n",
                "    func=litellm.acompletion,\n",
                "    func_name=\"async_completion\",\n",
                "    func_cache_name=\"completion\",\n",
                "    module_name=__name__,\n",
                "    cache_key_content_args=['messages', 'response_format'],\n",
                "    retrieve_log_data=lambda model, func_kwargs, response, cache_args: {\n",
                "        \"method\": \"completion\",\n",
                "        \"input_tokens\": token_counter(model=model, messages=func_kwargs['messages'], **cache_args),\n",
                "        \"output_tokens\": sum([token_counter(model=model, messages=[{'role': c.message.role, 'content': c.message.content}], **cache_args) for c in response.choices]),\n",
                "        \"cost\": response._hidden_params['response_cost'],\n",
                "    }\n",
                ")\n",
                "\n",
                "completion.__doc__ = \"\"\"\n",
                "This function is a wrapper around a corresponding function in the `litellm` library, see [this](https://docs.litellm.ai/docs/completion/input) for a full list of the available arguments.\n",
                "\"\"\".strip()\n",
                "\n",
                "sig = inspect.signature(async_completion)\n",
                "sig = sig.replace(parameters=[\n",
                "    Parameter(\"model\", Parameter.POSITIONAL_OR_KEYWORD, annotation=str),\n",
                "    Parameter(\"messages\", Parameter.POSITIONAL_OR_KEYWORD, annotation=List[Dict[str, str]]),\n",
                "    *sig.parameters.values()\n",
                "])\n",
                "async_completion.__signature__ = sig"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "162a84af",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'The capital of France is Paris.'"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "response, cache_hit, call_log = await async_completion(\n",
                "    model=\"gpt-3.5-turbo\",\n",
                "    messages=[\n",
                "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
                "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
                "    ],\n",
                ")\n",
                "response.choices[0].message.content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "86c7b91a",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "## single\n",
                            "\n",
                            "```python\n",
                            "single(\n",
                            "   prompt: str,\n",
                            "   model: str | None,\n",
                            "   system: str | None,\n",
                            "   *args,\n",
                            "   multi: typing.Union[bool, typing.Dict, NoneType],\n",
                            "   return_full_response: bool,\n",
                            "   return_info: bool,\n",
                            "   **kwargs\n",
                            ")\n",
                            "```\n",
                            "\n",
                            "Simplified chat completions designed for single-turn tasks like classification, summarization, or extraction. For a full list of the available arguments see the [documentation](https://docs.litellm.ai/docs/completion/input) for the `completion` function in `litellm`.\n",
                            "\n",
                            "If 'return_info' is set to True, the function returns a tuple of the response, cache hit status, and call log. If set to False, it returns only the response.\n",
                            "If 'multi' is provided, it should be a dictionary containing the model and messages from previous turns, allowing for multi-turn interactions. The function will append the new prompt to the existing messages.\n",
                            "\n",
                            "---\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#|hide\n",
                "show_doc(this_module.single)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6e6021c3",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|exporti\n",
                "def _get_msgs(orig_msgs, response):\n",
                "    if len(response.choices) == 0: return orig_msgs\n",
                "    msgs = orig_msgs.copy()\n",
                "    # msgs.append(response.choices[0].message.model_dump())\n",
                "    msgs.append({ 'role': response.choices[0].message.role, 'content': response.choices[0].message.content })\n",
                "    return msgs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "550e2ec6",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "def single(\n",
                "    prompt: str,\n",
                "    model: str|None = None,\n",
                "    system: str|None = None,\n",
                "    *args,\n",
                "    multi: bool|Dict|None = None,\n",
                "    return_full_response: bool=False,\n",
                "    return_info: bool=True,\n",
                "    **kwargs,\n",
                "):\n",
                "    if system is None and multi is None: system = \"You are a helpful assistant.\"\n",
                "    if system is not None and type(multi) == dict: raise ValueError(\"Cannot provide `system` if already in multi-turn completion mode.\")\n",
                "    if multi: model = model or multi['model']\n",
                "    \n",
                "    if type(multi) == dict:\n",
                "        messages = multi['messages'].copy() + [ {\"role\": \"user\", \"content\": prompt} ]\n",
                "    else:\n",
                "        messages = [\n",
                "            {\"role\": \"system\", \"content\": system},\n",
                "            {\"role\": \"user\", \"content\": prompt}\n",
                "        ]\n",
                "    \n",
                "    if model is None: raise ValueError(\"`model` must be provided.\")\n",
                "    \n",
                "    response, cache_hit, call_log = completion(model, messages, *args, **kwargs)\n",
                "    \n",
                "    res = response.choices[0].message.content if not return_full_response else response\n",
                "    if multi is not None:\n",
                "        res = res, {'model' : model, 'messages' : _get_msgs(messages, response)}\n",
                "    \n",
                "    return res, cache_hit, call_log if return_info else res\n",
                "    \n",
                "single.__name__ = \"single\"\n",
                "single.__doc__ = \"\"\"\n",
                "Simplified chat completions designed for single-turn tasks like classification, summarization, or extraction. For a full list of the available arguments see the [documentation](https://docs.litellm.ai/docs/completion/input) for the `completion` function in `litellm`.\n",
                "\n",
                "If 'return_info' is set to True, the function returns a tuple of the response, cache hit status, and call log. If set to False, it returns only the response.\n",
                "If 'multi' is provided, it should be a dictionary containing the model and messages from previous turns, allowing for multi-turn interactions. The function will append the new prompt to the existing messages.\n",
                "\"\"\".strip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fcd5c110",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'The capital of France is Paris.'"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "response, cache_hit, call_log = single(\n",
                "    model='gpt-4o-mini',\n",
                "    system='You are a helpful assistant.',\n",
                "    prompt='What is the capital of France?',\n",
                ")\n",
                "response"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "efb9f8b7",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "Recipe(name='Simple Pancakes', ingredients=['1 cup all-purpose flour', '2 tablespoons sugar', '2 teaspoons baking powder', '1/2 teaspoon salt', '1 cup milk', '1 egg', '2 tablespoons melted butter', '1 teaspoon vanilla extract'], steps=['In a large bowl, whisk together the flour, sugar, baking powder, and salt.', 'In a separate bowl, mix the milk, egg, melted butter, and vanilla extract until well combined.', \"Pour the wet ingredients into the dry ingredients and stir until just combined. Do not overmix; it's okay if there are a few lumps.\", 'Heat a non-stick skillet or griddle over medium heat and grease lightly with butter or oil.', 'Pour 1/4 cup of batter onto the skillet for each pancake. Cook until bubbles form on the surface, about 2-3 minutes.', 'Flip the pancakes and cook for another 2-3 minutes, until golden brown.', 'Remove from skillet and keep warm while cooking the remaining pancakes.'])"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "class Recipe(BaseModel):\n",
                "    name: str\n",
                "    ingredients: List[str]\n",
                "    steps: List[str]\n",
                "\n",
                "response, cache_hit, call_log = single(\n",
                "    model=\"gpt-4o-mini\",\n",
                "    system=\"You are a helpful cooking assistant.\",\n",
                "    prompt=\"Give me a simple recipe for pancakes.\",\n",
                "    response_format=Recipe\n",
                ")\n",
                "\n",
                "Recipe.model_validate_json(response)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ae6a986d",
            "metadata": {},
            "source": [
                "Can do multi-turn completions using `get_msgs=True` and passing the messages to the `prev` argument:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eaa5c6c9",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1 plus 1 equals 2.\n",
                        "2 multiplied by 10 equals 20.\n"
                    ]
                }
            ],
            "source": [
                "(res, _ctx), cache_hit, call_log = single(\n",
                "    model='gpt-4o-mini',\n",
                "    system='You are a helpful assistant.',\n",
                "    prompt='Add 1 and 1',\n",
                "    multi=True\n",
                ")\n",
                "print(res)\n",
                "\n",
                "(res, _ctx), cache_hit, call_log = single(\n",
                "    prompt='Multiply that by 10',\n",
                "    multi=_ctx,\n",
                ")\n",
                "print(res)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4530b135",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "## async_single *(async)*\n",
                            "\n",
                            "```python\n",
                            "async_single(\n",
                            "   prompt: str,\n",
                            "   model: str | None,\n",
                            "   system: str | None,\n",
                            "   *args,\n",
                            "   multi: typing.Union[bool, typing.Dict, NoneType],\n",
                            "   return_full_response: bool,\n",
                            "   return_info: bool,\n",
                            "   **kwargs\n",
                            ")\n",
                            "```\n",
                            "\n",
                            "Simplified chat completions designed for single-turn tasks like classification, summarization, or extraction. For a full list of the available arguments see the [documentation](https://docs.litellm.ai/docs/completion/input) for the `completion` function in `litellm`.\n",
                            "\n",
                            "If 'return_info' is set to True, the function returns a tuple of the response, cache hit status, and call log. If set to False, it returns only the response.\n",
                            "If 'multi' is provided, it should be a dictionary containing the model and messages from previous turns, allowing for multi-turn interactions. The function will append the new prompt to the existing messages.\n",
                            "\n",
                            "---\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#|hide\n",
                "show_doc(this_module.async_single)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "42430b79",
            "metadata": {},
            "outputs": [],
            "source": [
                "#|export\n",
                "async def async_single(\n",
                "    prompt: str,\n",
                "    model: str|None = None,\n",
                "    system: str|None = None,\n",
                "    *args,\n",
                "    multi: bool|Dict|None = None,\n",
                "    return_full_response: bool=False,\n",
                "    return_info: bool=True,\n",
                "    **kwargs,\n",
                "):\n",
                "    if system is None and multi is None: system = \"You are a helpful assistant.\"\n",
                "    if system is not None and type(multi) == dict: raise ValueError(\"Cannot provide `system` if already in multi-turn completion mode.\")\n",
                "    if multi: model = model or multi['model']\n",
                "    \n",
                "    if type(multi) == dict:\n",
                "        messages = multi['messages'].copy() + [ {\"role\": \"user\", \"content\": prompt} ]\n",
                "    else:\n",
                "        messages = [\n",
                "            {\"role\": \"system\", \"content\": system},\n",
                "            {\"role\": \"user\", \"content\": prompt}\n",
                "        ]\n",
                "    \n",
                "    if model is None: raise ValueError(\"`model` must be provided.\")\n",
                "    \n",
                "    response, cache_hit, call_log = await async_completion(model, messages, *args, **kwargs)\n",
                "    \n",
                "    res = response.choices[0].message.content if not return_full_response else response\n",
                "    if multi is not None:\n",
                "        res = res, {'model' : model, 'messages' : _get_msgs(messages, response)}\n",
                "    \n",
                "    return res, cache_hit, call_log if return_info else res\n",
                "    \n",
                "async_single.__name__ = \"async_single\"\n",
                "async_single.__doc__ = \"\"\"\n",
                "Simplified chat completions designed for single-turn tasks like classification, summarization, or extraction. For a full list of the available arguments see the [documentation](https://docs.litellm.ai/docs/completion/input) for the `completion` function in `litellm`.\n",
                "\n",
                "If 'return_info' is set to True, the function returns a tuple of the response, cache hit status, and call log. If set to False, it returns only the response.\n",
                "If 'multi' is provided, it should be a dictionary containing the model and messages from previous turns, allowing for multi-turn interactions. The function will append the new prompt to the existing messages.\n",
                "\"\"\".strip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b5a93aa1",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'The capital of France is Paris.'"
                        ]
                    },
                    "execution_count": null,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "response, cache_hit, call_log = await async_single(\n",
                "    model='gpt-4o-mini',\n",
                "    system='You are a helpful assistant.',\n",
                "    prompt='What is the capital of France?',\n",
                ")\n",
                "response"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2a38053f",
            "metadata": {},
            "source": [
                "You can execute a batch of prompt calls using `adulib.asynchronous.batch_executor`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e1e1ac2a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "The capital of France is Paris.\n",
                        "The capital of Germany is Berlin.\n",
                        "The capital of Italy is Rome.\n",
                        "The capital of Spain is Madrid.\n",
                        "The capital of Portugal is Lisbon.\n"
                    ]
                }
            ],
            "source": [
                "results = await batch_executor(\n",
                "    func=async_single,\n",
                "    constant_kwargs=as_dict(model='gpt-4o-mini', system='You are a helpful assistant.'),\n",
                "    batch_kwargs=[\n",
                "        { 'prompt': 'What is the capital of France?' },\n",
                "        { 'prompt': 'What is the capital of Germany?' },\n",
                "        { 'prompt': 'What is the capital of Italy?' },\n",
                "        { 'prompt': 'What is the capital of Spain?' },\n",
                "        { 'prompt': 'What is the capital of Portugal?' },\n",
                "    ],\n",
                "    concurrency_limit=2,\n",
                "    verbose=False,\n",
                ")\n",
                "\n",
                "print(\"\\n\".join([response for response, _, _ in results]))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "adulib",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        },
        "nblite_source_hash": "e62647aef830145739e64a93043549fded7aca0b7a9a0b944982a09ff9db0467"
    },
    "nbformat": 4,
    "nbformat_minor": 5
}